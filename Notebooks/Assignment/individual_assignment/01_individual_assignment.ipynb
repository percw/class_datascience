{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795860f2-eabf-40d1-954c-843dcf672a7d",
   "metadata": {},
   "source": [
    "# MGT-499 Statistics and Data Science - Individual Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a58faf4-db28-41c1-9e6d-ead96d328eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import here what you need\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5761d-3238-4e69-951f-6fea4557ef41",
   "metadata": {},
   "source": [
    "This notebook contains the individual assignment for the class MGT-499 Statistics and Data Science. Important information:\n",
    "- **Content**: the assignment is divided in two main parts, namely data cleaning (2 datasets) and Exploratory Data Analysis, for a total of 13 main questions (see table of contents). Some of these main questions are divided in sub questions. In the first part, the questions are very specific, while in the second part they are more open.\n",
    "- **Deadline**: Tuesday 8th of November at 23:59. \n",
    "- **Final Output**: a Jupyter notebook, which we (teachers) can run. \n",
    "- **Answering the Questions**: you will find the questions in markdown cells below. Under each of these cells, you will find a cell / cells for answers. Type there your answer. For the answer to be correct, the cell with the answer must run without error (unless specified). You can use markdown cells for the answers that require text.\n",
    "- **Submission**: submit the assignment on Moodle, under [Individual Assignment](https://moodle.epfl.ch/mod/assign/view.php?id=1222846)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e43ccf-9ed6-44ea-8dd3-184cb9e7c499",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Content\n",
    "- [Polity5 Dataset](#polity5)  \n",
    "    - [Question 1: Import the data and get a first glance](#question1)\n",
    "    - [Question 2: Select some variables](#question2)\n",
    "    - [Question 3: Missing Values](#question3)\n",
    "    - [Question 4: Check Polity2](#question4)\n",
    "- [Quality of Government (QOG) Environmental Indicators Dataset](#qog)  \n",
    "    - [Question 5: Import the data and do few fixes](#question5)\n",
    "    - [Question 6: Merge QOG and Polity5 ... first attempt](#question6)\n",
    "    - [Question 7: Merge QOG and Polity5 ... second attempt](#question7)\n",
    "    - [Question 8: Clean the merged dataframe](#question8)\n",
    "- [Exploratory Data Analysis](#eda)\n",
    "    - [Question 9: Selecting the ingredients for the recipe (how I select the variables)](#question9)  \n",
    "    - [Question 10: Picking the right quantity of each ingredient (how I select my sample)](#question10)\n",
    "    - [Question 11: Tasting and preparing the ingredients (univariate analysis)](#question11)\n",
    "    - [Question 12: Cooking the ingredients together (bivariate analysis)](#question12)\n",
    "    - [Question 13: Tasting the new recipe (conclusion)](#question13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142f738-6239-4e12-9c65-fd27b4db73ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Polity5 data <a class=\"anchor\" id=\"polity5\"></a>\n",
    "\n",
    "Polity5 is a widely used democracy scale. The raw data as well as the codebook are available [here](http://www.systemicpeace.org/inscrdata.html). For this assignment, we have modified a bit the original version, for example we have added the iso3 code for countries to make you save time. You can find the modified version [here](https://raw.githubusercontent.com/edoardochiarotti/class_datascience/main/Notebooks/Assignment/individual_assignment/data/polity2_iso3.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903da7f-480e-4777-a4ef-9ab94b09e473",
   "metadata": {},
   "source": [
    "### Question 1: import the data and get a first glance <a class=\"anchor\" id=\"question1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f6829-6c8f-451a-9670-4d1c7a103c12",
   "metadata": {},
   "source": [
    "1a) Import the csv 'polity2_iso3.csv' (file provided in the link [here](https://raw.githubusercontent.com/edoardochiarotti/class_datascience/main/Notebooks/Assignment/individual_assignment/data/polity2_iso3.csv)) as a panda dataframe (ignore the warning message) **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e4014-e55c-4994-a633-61a8fdf1e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1a\n",
    "url = 'https://raw.githubusercontent.com/edoardochiarotti/class_datascience/main/Notebooks/Assignment/individual_assignment/data/polity2_iso3.csv'\n",
    "polity_data = pd.read_csv(url, low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd738c61-10ca-4439-8bcf-20e6987e3e5f",
   "metadata": {},
   "source": [
    "1b) Display the first 10 rows **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10544e0c-33b5-4b90-ad68-4cfe8d6c40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1b\n",
    "polity_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6decfca-afb8-4c77-881c-8fb78d96e68d",
   "metadata": {},
   "source": [
    "1c) Display the data types of all the variables included in the data **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa768541-03f9-424a-a690-d6132298f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1c\n",
    "polity_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ee31a-7cc9-44d2-b7f8-242f6ee5ebfc",
   "metadata": {},
   "source": [
    "1d) By looking at your answer in 1c, what is the difference between the different types of variables? Why the type of some variables is defined as object? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97f628-71b6-4aac-9c9c-596773557131",
   "metadata": {},
   "source": [
    "Answer 1d:\n",
    "\n",
    "We can see that we have four different dataypes: `object`, `float64` and `int64`. `Object` is  the pandas type for mixed values s.a. strings and numbers. In python the equivivalent  type is `string`. `Float64` refers to numeric characters with decimals. If columns contain numbers and NaN pandas will defualt to `float64`. And for `int64` is for integer numbers, and the 64 refers to memory allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfba84f-b897-4fcb-880d-9d8995045239",
   "metadata": {},
   "source": [
    "### Question 2. Select some variables <a class=\"anchor\" id=\"question2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9776a-26ed-4154-af73-0e2b337e9e74",
   "metadata": {},
   "source": [
    "2a) Create a subset dataframe that contains the variables 'iso3', 'country', 'year', 'polity2' and display it **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd0a39-386d-4397-93eb-8eb7e0415a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2a\n",
    "polity_data_subset = (polity_data[['iso3', 'country', 'year', 'polity2']]).copy()\n",
    "polity_data_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4883d-8f9e-4b1f-9ce4-c62267090287",
   "metadata": {},
   "source": [
    "2b) Display the type of the variable \"year\" **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc6b73-70d9-42c0-9cd1-716033325a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2b\n",
    "polity_data_subset['year'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639b2d3-b5e6-40f3-93c9-6547bcdcb814",
   "metadata": {},
   "source": [
    "2c) Convert the variable \"year\" to string **(1 point)**\n",
    "<br>\n",
    "Hint: if you get a warning message of the type \"SettingWithCopyWarning\", it is because you did not subset the data in the right way. Go back to your class notes and check the different ways to subset a dataframe, and try again. If you do it correctly, you will not get the warning message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d8f60-9875-40be-9f78-0c7c2837a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2c\n",
    "polity_data_subset['year'] = polity_data_subset['year'].astype('string')\n",
    "polity_data_subset['year'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0babf3-0e33-4e13-9289-523ecef5e004",
   "metadata": {},
   "source": [
    "### Question 3: Missing Values <a class=\"anchor\" id=\"question3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb19a0f-bd7c-4d9c-bdfa-05e55e4eb26f",
   "metadata": {},
   "source": [
    "3a) Subset the rows that have iso3 missing and display **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb48bb3-a92a-4565-8a5c-f22f642f2661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3a\n",
    "iso_missing = polity_data_subset[polity_data_subset['iso3'].isna()]\n",
    "iso_missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4b352b-c860-4b2c-bca3-5437c2bbaf23",
   "metadata": {},
   "source": [
    "3b) Display the countries that have missing iso3. What can you tell by looking at them? Any similarities? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9760a-a4a6-4826-b377-49ff300662f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3b\n",
    "iso_missing_contries = iso_missing['country']\n",
    "print(f'They seem to be old countries, or old country names.\\n{iso_missing_contries.unique()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41b9a2-2c70-41c6-b891-95e01f2a74a5",
   "metadata": {},
   "source": [
    "3c) Display the countries with missing iso3 from 2011. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408fff2-6493-4dff-9294-ef1c564315fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3c\n",
    "# Interpreting the question so that we take the contries missing iso3 from 2011 onwards\n",
    "global df_missing_iso3_since_2011\n",
    "global df_missing_iso3_since_2011_list\n",
    "\n",
    "df_missing_iso3_since_2011 = iso_missing[iso_missing['year'].astype(int) >= 2011]\n",
    "df_missing_iso3_since_2011_list = sorted(df_missing_iso3_since_2011['country'].unique())\n",
    "df_missing_iso3_since_2011_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd9a6b6-2151-4c5e-8a8a-ea9810905642",
   "metadata": {},
   "source": [
    "3d) Display the rows for which the column \"country\" contains the word \"Serbia\". By looking at the result, can you tell what happened to Serbia in 2006? **(1 point)**\n",
    "<br>\n",
    "Hint: the most general way of doing this is to use a combination of re.search and list comprehension. To display the full subset, you can use print(df.to_string())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953ca64-d67b-4c94-8df6-5077efa68814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3d\n",
    "\n",
    "# It is important to check for varieties of capilized firstletter when using case sensitive comparison. Information may be lost. Here all country instances uses first capital letter.\n",
    "polity_data_subset_serbia = polity_data_subset[polity_data_subset['country'].str.contains(pat='Serbia', case=False)]\n",
    "print(polity_data_subset_serbia.to_string())\n",
    "\n",
    "# We can see that the country referred to as Serbia, reappeared in 2006, this is due to a name change following Montenegro and Serbias decleration of independence of their previous union named `Serbia and Montenegro`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850dd97b-0aaa-4bca-affb-22f3c0ca387c",
   "metadata": {},
   "source": [
    "3e) Write a function that does the operation in 3d and use it to display the subset that has the word \"sudan\" (all lower cap) in country. Then do the same for the word \"vietnam\" (all lower cap). **(1 point)**\n",
    "<br>\n",
    "Hint: options of functions can be very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217b330-72ef-4d38-8a66-1849d12f7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3e\n",
    "#\n",
    "# Function find_country\n",
    "# \tinput: dataframe, column, name\n",
    "# \toutput: df containting the specified string in the column 'country'\n",
    "\n",
    "def find_rows_with_pattern(df:pd.DataFrame, column:str, name:str):\n",
    "\ttry:\n",
    "\t\treturn df[df[column].str.contains(pat=name, case=False)]\n",
    "\texcept TypeError as e:\n",
    "\t\tprint(f'Not a valid datatype: {e}.\\nUse {pd.DataFrame} and {str} as option type parameters')\n",
    "\n",
    "df_sudan = find_rows_with_pattern(polity_data_subset, 'country', 'sudan')\n",
    "df_vietnam = find_rows_with_pattern(polity_data_subset, 'country', 'vietnam')\n",
    "\n",
    "print(df_sudan.to_string())\n",
    "print(df_vietnam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a0adc-6104-4dff-baa4-cbc8259183b4",
   "metadata": {},
   "source": [
    "3f) Replace nan values in iso3 with correct iso3 for the 5 countries found in 3c from 2011 onwards, and display the subset with the fixed values to check that everything worked. **(1 point)**\n",
    "<br>\n",
    "Hint: the correct iso3 for these 5 countries are \"ETH\",\"MNE\",\"SRB\",\"SDN\",\"VNM\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab9532-5a03-46fe-9db7-170e8b087957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3f\n",
    "\n",
    "# Storing the correct iso3 values in a list\n",
    "iso3_code_missing = ('ETH', 'MNE', 'SRB', 'SDN', 'VNM')\n",
    "\n",
    "# Creating a dict to look up correct iso3 values with list from 3c\n",
    "iso3_dict = dict(zip(df_missing_iso3_since_2011_list, iso3_code_missing))\n",
    "\n",
    "# Copying the politydata to work on\n",
    "polity_data_subset_iso_replace = polity_data_subset.copy()\n",
    "\n",
    "# Function: replace(df, dct, replaced)\n",
    "# input: df: dataframe, dct: dictionary to loop through, replaced: item to be replaced\n",
    "# manipulates in place\n",
    "\n",
    "def replace(df, dct, replaced, from_year):\n",
    "    for key, item in dct.items():\n",
    "        df.loc[(df['country'] == key) & (df['year'].astype(int) >= from_year)] = df.loc[(\n",
    "            df['country'] == key) & (df['year'].astype(int) >= 2011)].replace(replaced, item)\n",
    "\n",
    "\n",
    "# Calling the function and telling which value i want to replace\n",
    "replace(polity_data_subset_iso_replace, iso3_dict, np.nan, 2011)\n",
    "\n",
    "# Using the indexes of the subset created in 3c to check if we have the correct countrycodes\n",
    "polity_data_subset_iso_replace.iloc[df_missing_iso3_since_2011.index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa210c-b46f-46ff-a8da-d72d95bff196",
   "metadata": {},
   "source": [
    "3g) Drop the remaining rows which have nan in \"iso3\" and display the new number of rows of the dataframe (how many are they?) **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2cea78-0af1-405f-aeed-41dfc8fee4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3g\n",
    "\n",
    "# Dropping NaNs in the iso3 column\n",
    "polity_data_ss_cleaner = polity_data_subset_iso_replace.dropna(subset='iso3')\n",
    "\n",
    "# Number of rows, and making sure that the data is consistent\n",
    "print(f'Before removing rows with NaNs in column \"iso3\" we had: {len(polity_data_subset_iso_replace)} rows')\n",
    "print(f'Now we have: {len(polity_data_ss_cleaner)} rows')\n",
    "print(f'Which means that we removed: {len(polity_data_subset_iso_replace) - len(polity_data_ss_cleaner)} rows')\n",
    "print(f'Which makes sense since we had {polity_data_subset_iso_replace[\"iso3\"].isna().sum()} instances of NaNs in the polity2 column.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313dc318-2264-4e42-8ca3-f1cf5b85fac5",
   "metadata": {},
   "source": [
    "### Question 4: Check Polity2 <a class=\"anchor\" id=\"question4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41d643-6867-4c1c-93f9-c3c38fb9d454",
   "metadata": {},
   "source": [
    "4a) Display the first and last year included in the dataset **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57202c70-fe45-4f45-a29b-6a4761421972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4a\n",
    "\n",
    "# Assuming the question is interpreted so that we are to find earliest and latest year recorded we have:\n",
    "print(f'Earliest entry year: {polity_data[\"year\"].min()}, Latest entry year: {polity_data[\"year\"].max()}')\n",
    "\n",
    "# Assuming the assignment is asking for the first and last year in the first and last row of the original dataset\n",
    "print(f'First row year: {polity_data.iloc[[0, -1]][\"year\"].values[0]}, and last row year: {polity_data.iloc[[0, -1]][\"year\"].values[-1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4593d98-be9b-45b3-86df-82e613ee851b",
   "metadata": {},
   "source": [
    "4b) What do the values in \"polity2\" represent? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1c4ecc-c2e6-4150-bf91-86bc067fd4a7",
   "metadata": {},
   "source": [
    "Answer 4b: \n",
    "\n",
    "`Polity2` is a revised version of the Polity score, which captures a regime authority on a `21-point` scale ranging from `-10` (hereditary monarchy) to `10` (consolidated democracy). `-66` are cases of foreign interruption and are treated as missing values. Whearas `-88` represents transitions. If a given country has the value of `-5` i 1990 and `+5` in 2000, it means that it has an annual increase of `+1`; and the converted scores are `1991: -4 1992: -3 ... 2000: +5` \n",
    "\n",
    "The documentation I used is [here](https://www.systemicpeace.org/inscr/p4manualv2016.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a9072-8271-4336-9219-3c64a3f16e07",
   "metadata": {},
   "source": [
    "4c) Do we have weird values for polity2? If yes, why? What should we do about them? Transform the data accordingly. **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e710121-c355-4f34-8f81-187ccee789b7",
   "metadata": {},
   "source": [
    "Answer 4c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40f8d3-e7bd-49b9-9faa-a76931d0f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4c\n",
    "\n",
    "# We can see that we have NaNs in polity2\n",
    "print(f'Number of NaNs in the df per column: \\n {polity_data_ss_cleaner.isna().sum()}')\n",
    "print(f'\\nThe unique values of polity2: \\n {polity_data_ss_cleaner.polity2.unique()}')\n",
    "\n",
    "# To repeat:\n",
    "# -66 : cases of foreign “interruption” are treated as “system missing.”\n",
    "# -88 : Cases of “transition” are prorated across the span of the transition. \n",
    "# So the NaNs may be there for a reason...\n",
    "\n",
    "# Creating a copy df to work on\n",
    "df_cleaned = polity_data_ss_cleaner.copy()\n",
    "\n",
    "# Lets find the rows with -66 and -88\n",
    "polity_66_88_index = df_cleaned[(df_cleaned.polity2 == -66) | (df_cleaned.polity2 == -88)].index\n",
    "print(f'\\nRows with -66 and -88:\\n{polity_data_subset.iloc[polity_66_88_index].to_string()}')\n",
    "\n",
    "# In addition -88 values should be treated as missing values, as i have looked into the data for other occupied countries during the world wars\n",
    "# We see that they have NaNs and not -66. I will therefor change the -66 to NaN\n",
    "df_cleaned.loc[df_cleaned.polity2 == -66] = df_cleaned.loc[(df_cleaned.polity2 == -66)].replace(-66, 0)\n",
    "\n",
    "# As Belgium has -88 for the first row, and all the nearest following years has score -4 i will change this value to -4\n",
    "df_cleaned.loc[df_cleaned.polity2 == -88] = df_cleaned.loc[(df_cleaned.polity2 == -88)].replace(-88, np.float64(-4))\n",
    "\n",
    "# Printing the current unique polity2 values to verify changes\n",
    "print(f'\\nThe new unique values of polity2: \\n {df_cleaned.polity2.unique()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b6f3a-11d9-477f-9fab-f7e644176c67",
   "metadata": {},
   "source": [
    "4d) Make a map that shows the number of observations of polity2 by country **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53846332-57ef-4909-8367-5ffd2d737eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4d\n",
    "\n",
    "pol_obs_per_country = df_cleaned.groupby(['iso3'])['polity2'].size().reset_index()\n",
    "pol_obs_map = px.choropleth(pol_obs_per_country, locations='iso3',\n",
    "                    title='Number of polity2 observations by country',\n",
    "                    locationmode='ISO-3',\n",
    "                    color='polity2', \n",
    "                    hover_name='iso3',\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma)\n",
    "\n",
    "pol_obs_map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5258b-9ef6-4e9d-bf52-29a0d7ebc9b7",
   "metadata": {},
   "source": [
    "4e) Store the final dataframe (the one you obtained after 5d) in an object called df_pol **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb74cbc-5356-4e4b-b5b4-a5174c9ced76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4e\n",
    "df_pol = df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b6cd3-101d-4343-b76d-d97583fe33ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Quality of Government Environmental Indicators <a class=\"anchor\" id=\"qog\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58204e-f401-4b6e-b64a-c9d098faa0f2",
   "metadata": {},
   "source": [
    "The QoG Environmental Indicators dataset (QoG-EI) (Povitkina, Marina, Natalia Alvarado Pachon & Cem Mert Dalli. 2021). The Quality of Government Environmental Indicators Dataset, version Sep21. University of Gothenburg: The Quality of Government Institute, https://www.gu.se/en/quality-government), is a compilation of indicators measuring countries' environmental performance over time, including the presence and stringency of environmental policies, environmental outcomes (emissions, deforestation, etc.), and public opinion on the environment. Codebook and data are available [here](https://www.gu.se/en/quality-government/qog-data/data-downloads/environmental-indicators-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc77f2-25d1-41ce-930a-c25a05ac9cdc",
   "metadata": {},
   "source": [
    "### Question 5: Import the data and do few fixes <a class=\"anchor\" id=\"question5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959796a-3340-4231-838c-a4e2cba4262c",
   "metadata": {},
   "source": [
    "5a) Import data from the Quality of Government Environmental Indicators Dataset and display the variables types and the number of rows **(1 point)**\n",
    "<br>\n",
    "Hint: When you go on the webpage of the Environmental Indicators Dataset, you can directly import from a URL by copying the link address of the dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ef767-dca9-426e-9e34-1335038b3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5a\n",
    "url = 'https://www.qogdata.pol.gu.se/data/qog_ei_sept21.csv'\n",
    "df_qog = pd.read_csv(url, encoding='latin-1')\n",
    "\n",
    "print(f'Variable types: {df_qog.dtypes} \\n Number of rows: {len(df_qog)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a8293-d098-4a79-a803-50d52f6b63b3",
   "metadata": {},
   "source": [
    "5b) Rename the variable \"ccodealp\" to \"iso3\" **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e78d3-06b3-418c-81b8-05bbef009ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5b\n",
    "df_qog.rename(columns={'ccodealp': 'iso3'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b34e8-0daa-4481-9b1b-61e7d318c571",
   "metadata": {},
   "source": [
    "5c) Check the type of the variables \"year\" and \"iso3\" are string, if not convert them to string **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1993423-8edf-4bb0-8ea0-69034fce5a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5c\n",
    "print(f'Before convert \\nYear datatype: {df_qog.year.dtypes}')\n",
    "print(f'iso3 datatype: {df_qog.iso3.dtypes}')\n",
    "\n",
    "df_qog[['year', 'iso3']] = df_qog[['year', 'iso3']].astype('string')\n",
    "\n",
    "print(f'After convert \\nYear datatype: {df_qog.year.dtypes}')\n",
    "print(f'iso3 datatype: {df_qog.iso3.dtypes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b61d4-55aa-439b-823f-67aad3f6fc16",
   "metadata": {},
   "source": [
    "### Question 6: Merge QOG and Polity5 ... issues with QOG? <a class=\"anchor\" id=\"question6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8af7f-3b38-4834-90b4-e1cbbb7b7359",
   "metadata": {},
   "source": [
    "6a) Get a subset of the dataframe that includes the variables \"cname\", \"iso3\", \"year\" and \"cckp_temp\", and display the number of rows. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736cd36-ed4e-4dbd-96a9-c3d51bd343be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6a\n",
    "\n",
    "df_qog_subset = df_qog[['cname', 'iso3',  'year', 'cckp_temp']]\n",
    "len(df_qog_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a0373-fab5-4346-bed7-3500ed9aced0",
   "metadata": {},
   "source": [
    "6b) Merge this subset (left) and the clean version of the polity data (right), using the argument how=\"left\". Was the merge succesfull? If yes, how many rows has the merged dataframe? Is it the same number of rows of the subset in 6a? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2b0ba-fdd7-474a-9241-6d96e6eca6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6b\n",
    "\n",
    "merged = pd.merge(left=df_qog_subset, right=df_pol, how='left')\n",
    "print(f'With {len(merged)} rows, its the same amount of rows at we saw in 6a')\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e5a50-5bda-4331-82a9-9cfe09a365cb",
   "metadata": {},
   "source": [
    "6c) Do the same by adding the argument validate=\"one-to-one\". Can you make some hypotheses on why you get an error? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8e0a9-75e7-4c37-80ad-3f5ddef8d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6c\n",
    "\n",
    "# By using the option validate='one-to-one', we get an error message which states that the merge keys are not unique. \n",
    "# Which makes sense which we for example have several rows in the same column with the same value.\n",
    "merge_one_to_one = pd.merge(left=df_qog_subset, right=df_pol, validate='1:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8da9ac-de08-460e-b364-40aaf5a9aa34",
   "metadata": {},
   "source": [
    "6d) Consider the subset of the QOG you obtained in 6a and write a code to (i) count the number of observations for the variable \"cckp_temp\" for each combination of iso3 and year, (ii) store the results in a dataframe. For example, the combination \"USA-2012\" should have 1 observation for \"cckp_temp\", so the result of your code should be 1. The code should do this for all iso3-year combinations of your subset dataframe, and store the results in a dataframe. **(1 point)**\n",
    "<br>\n",
    "Hint: it should not take you more than 2 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a4be8-0760-454d-8f55-e5e0bd18af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6d\n",
    "cckp_temp_per_combination = df_qog_subset.groupby(['iso3', 'year'])['cckp_temp'].size().reset_index()\n",
    "cckp_temp_per_combination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e92628-2085-4edf-aed4-8e6db647d9c6",
   "metadata": {},
   "source": [
    "6e) Use the code in 6d to write a function that displays all rows of the dataframe obtained in 6a that have more than one observation of \"cckp_temp\" for each iso3-year combination, and check if it works. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953896a-28a5-4d38-8ad3-93027c5c4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6e\n",
    "def more_than_one_combination(df, c1, c2, obs):\n",
    "\tdf = df.groupby([c1, c2])[obs].size().reset_index()\n",
    "\tdf = df[df[obs].astype(int) > 1]\n",
    "\treturn df\n",
    "\n",
    "combination1 = 'iso3'\n",
    "combination2 = 'year'\n",
    "observation = 'cckp_temp'\n",
    "\n",
    "more_than_one_combination(df_qog_subset, combination1, combination2, observation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190da03-b34c-4566-8aec-5a1a540ac660",
   "metadata": {},
   "source": [
    "6f) Which countries have more than one observation for each iso3-year combination? Deal with these countries in the subset dataframe created in 6a to make sure you no longer have double observations for iso3-year combinations, and check that after your fix this is actually the case. **(1 point)**\n",
    "<br>\n",
    "Hint: should we keep a country with all missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d960c4a-b755-448c-8952-aee2e6eee878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6f\n",
    "\n",
    "# Using the previous function to get the countries with more than one iso3-year combination\n",
    "countries = more_than_one_combination(df_qog_subset, 'iso3', 'year', 'cckp_temp')\n",
    "\n",
    "# Getting the iso3 code for that particualar country\n",
    "iso3_duplicate_country = countries.iso3.unique()[0]\n",
    "\n",
    "# Creating a df to show all the data with the iso3-year combiantion constraints \n",
    "iso3_duplicate_df = df_qog_subset[df_qog_subset.iso3 == iso3_duplicate_country]\n",
    "\n",
    "# Inspecting the values, and seeing that all cckp_temp values for North Vietnam are NaN\n",
    "print(iso3_duplicate_df.to_string())\n",
    "\n",
    "# We saw that we only have NaN values for North Vietnam - so we lets find the indexes of these rows\n",
    "# In addiiton the Polity5 dont have any country names North Vietnam so i think we can remove these\n",
    "north_vietnam_indexes = df_qog_subset[df_qog_subset.cname == 'North Vietnam'].index\n",
    "\n",
    "# Based on the indexes we can drop these rows\n",
    "df_qog_cleaned = df_qog_subset.drop(index=north_vietnam_indexes)\n",
    "\n",
    "# Now we can check whether we have any duplicated by running the function from 6e again.\n",
    "more_than_one_combination(df_qog_cleaned,'iso3', 'year', 'cckp_temp').empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae70541-6427-4f83-bd05-37ef68baf5cd",
   "metadata": {},
   "source": [
    "6g) If your check went well, now you can perform the same operation directly in the QOG dataframe (not in the substed dataframe created in 6a). How many rows does now the QOG dataframe has? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f20b2-04e4-46d0-94f6-e580913ddfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6g\n",
    "\n",
    "# Using the function from 6e with the original dataset\n",
    "more_than_one_iso_year_combination_df = more_than_one_combination(df_qog, 'iso3', 'year', 'cckp_temp')\n",
    "\n",
    "# By inspecting the df it looks like we the same countries here. North Vietnam and Viet Nam.\n",
    "print(more_than_one_iso_year_combination_df.to_string())\n",
    "\n",
    "# Lets again find the indexes\n",
    "north_vietnam_indexes = df_qog[df_qog.cname == 'North Vietnam'].index\n",
    "\n",
    "# Dropping the North Vietnam indexes\n",
    "df_qog = df_qog.drop(index=north_vietnam_indexes)\n",
    "\n",
    "print(len(df_qog))\n",
    "\n",
    "# We can now see that we dont have any multiple combinations of iso3 and year anymore\n",
    "more_than_one_combination(df_qog, 'iso3', 'year', 'cckp_temp').empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd12e1-799e-4e4c-8d13-db496fb5bab4",
   "metadata": {},
   "source": [
    "### Question 7: Merge QOG and Polity5 ... issues with Polity5? <a class=\"anchor\" id=\"question7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72692a86-a9ab-44bb-bb56-b3c22e0829c4",
   "metadata": {},
   "source": [
    "7a) Merge the cleaned QOG dataframe (left) and the Polity dataframe (right) using the options how=\"left\" and validate=\"one_to_one\". Does it work? Why? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82261ca5-c3ec-49f6-8de8-be1b14943cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7a\n",
    "merge_qog_polity5 = pd.merge(left=df_qog, right=df_pol, how='left', validate='one_to_one')\n",
    "\n",
    "# We get the error message that the keys are not unique in the right dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95740ccf-88d7-46ee-82ab-f1a8b42d9722",
   "metadata": {},
   "source": [
    "7b) Use the function you wrote in 6e to check what's wrong in the \"clean\" version of Polity **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356409a0-73e8-41ac-af75-c2f6a462c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7b\n",
    "\n",
    "# By displaying the df_pol with iso3 value MNE we see that Kosovo has the wrong iso3 value. \n",
    "# This makes sense regarding the error message we got in 7a\n",
    "polity_combination_issues_polity = more_than_one_combination(df_pol, 'iso3', 'year', 'polity2')\n",
    "polity_combination_issues_country = more_than_one_combination(df_pol, 'iso3', 'year', 'country')\n",
    "print(f\"Checking which iso3-year combinations has several observations of polity2: \\n {polity_combination_issues_polity}\")\n",
    "print(f\"Checking which iso3-year combinations has several observations of countries: \\n {polity_combination_issues_country}\")\n",
    "print(f'Checking the countries with iso3 code MNE: {df_pol[df_pol.iso3 == \"MNE\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23d8f5b-10f5-4540-b714-99b77f3e5203",
   "metadata": {},
   "source": [
    "7c) Drop or fix the countries that create troubles directly in the \"clean\" version of Polity and motivate your choices. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb945c-418a-4eee-bdd3-2dce12246196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7c\n",
    "# Kosovo does not officially have a iso3 value but we can use XKX set by the European Commission\n",
    "# OR we can choose to drop it completely as the QoG dataset does not include data on Kosova at all\n",
    "# And since we only have values from 2011-2018 for Sudan-North and in QoG dataset dont have any\n",
    "# values for Sudan-North/North-Sudan/Sudan North/North Sudan at all, we should drop these.\n",
    "\n",
    "print(f'Checking if there are severeal countries with iso3 code SDN in QoG: {df_qog[df_qog.iso3 == \"SDN\"].cname.unique()}')\n",
    "print(f\"Checking if there is any data on Sudan-North in QoG: {find_rows_with_pattern(df_qog, 'cname', 'sudan-north').cname.unique()}\")\n",
    "print(f\"Checking if there is any data on North-Sudan in QoG: {find_rows_with_pattern(df_qog, 'cname', 'north-sudan').cname.unique()}\")\n",
    "print(f\"Checking if there is any data on North Sudan in QoG: {find_rows_with_pattern(df_qog, 'cname', 'sudan north').cname.unique()}\")\n",
    "print(f\"Checking if there is any data on Sudan North  in QoG: {find_rows_with_pattern(df_qog, 'cname', 'north sudan').cname.unique()}\")\n",
    "\n",
    "# I have devided to drop Kosovo and Sudan-North\n",
    "# Lets find the indexes of these rows\n",
    "kosovo_indexes = df_pol[df_pol.country == 'Kosovo'].index\n",
    "sudan_north_indexes = df_pol[df_pol.country == 'Sudan-North'].index\n",
    "\n",
    "# Based on the indexes we can drop these rows\n",
    "df_pol = df_pol.drop(index=kosovo_indexes)\n",
    "df_pol = df_pol.drop(index=sudan_north_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88282d66-3aa1-4fa4-b7cc-b448c649052c",
   "metadata": {},
   "source": [
    "7d) Try now to merge the \"clean-clean\" versions of qog and Polity (the ones you obtained in 7g and 8c) always using the options how=\"left\" and validate=\"one_to_one\". Does it work, and why? How many rows has the resulting merged dataframe? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e52b0f-7484-4656-82bd-8de4cee284a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7d\n",
    "merge_qog_polity5 = pd.merge(left=df_qog, right=df_pol, how='left', on=['iso3', 'year'], validate='one_to_one')\n",
    "len(merge_qog_polity5)\n",
    "\n",
    "# It works as there is no longer any ambiguioty with the combination of iso3 and year in either of the dataframes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8479c-556a-4c9b-b89f-a8f714e888ad",
   "metadata": {},
   "source": [
    "### Question 8: Clean the merged dataframe <a class=\"anchor\" id=\"question8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936854f-7882-47bc-a2c0-cb39b53128b0",
   "metadata": {},
   "source": [
    "8a) In the merged dataframe, order the columns so that you have the \"index\" variables first and the variables with actual values last. **(1 point)**\n",
    "<br>\n",
    "Hint: index variables are \"iso3\", \"year\" and other similar variables you can find, and the variables with actual values are \"polity2\", \"cckp_temp\" and other similar variables you can find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e92ad2-8007-4a4b-a47f-44c040ccfbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 8a\n",
    "\n",
    "# Displaying all the column names to get an overview\n",
    "column_list = merge_qog_polity5.columns.tolist()\n",
    "#print(column_list)\n",
    "\n",
    "# Popping the columns we want to re-order\n",
    "iso3_column = merge_qog_polity5.pop('iso3')\n",
    "year_column = merge_qog_polity5.pop('year')\n",
    "year_country = merge_qog_polity5.pop('country')\n",
    "\n",
    "# Inserting columns in the specified order\n",
    "merge_qog_polity5.insert(2, 'iso3', iso3_column)\n",
    "merge_qog_polity5.insert(3, 'year', year_column)\n",
    "merge_qog_polity5.insert(4, 'country', year_country)\n",
    "\n",
    "column_list_after = merge_qog_polity5.columns.tolist()\n",
    "#print(column_list_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b39c4e8-6f99-47c9-9d32-4196e4184537",
   "metadata": {},
   "source": [
    "8b) Rename \"cname\" as \"country\" and \"country\" as \"country_polity\". **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2cccd-473d-462e-9b2f-726aa6763d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 8b\n",
    "merge_qog_polity5.rename(columns={'cname': 'country', 'country': 'country_polity'}, inplace=True)\n",
    "merge_qog_polity5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353db63e-08d7-41df-9bdf-e0aa20174569",
   "metadata": {},
   "source": [
    "8c) Save the clean merged dataframe as a csv in a subfolder called \"clean_data\" in your working directory **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a684c82-6df7-480f-9374-a2d4e7ec069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 8c\n",
    "\n",
    "# Removing index columns\n",
    "#merge_qog_polity5 = merge_qog_polity5.pop('Unnamed: 0')\n",
    "\n",
    "# Cleaning path if exists\n",
    "if os.path.exists('clean_data'):\n",
    "\tos.remove('clean_data/clean_data.csv')\n",
    "\tos.rmdir('clean_data')\n",
    "\n",
    "# Creatning folder and exporting csv\n",
    "os.makedirs('clean_data')\n",
    "merge_qog_polity5.to_csv('clean_data/clean_data.csv', index=False)\n",
    "\n",
    "# I also want to add that i have done a lot of comparison check between the two csv files.\n",
    "# I learnt that using the .eq or .equeal method on float number does not work very well,\n",
    "# as these numbers are \"floating\", 1.0005 may in fact be 1.00005000102 when compared. \n",
    "# So i used np.isclose(a, b) - which works between a certain very small range.count\n",
    "# I'm happy to say that the two datasets looks identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f2840-c690-4b3e-8e4c-0025101bdc62",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis <a class=\"anchor\" id=\"eda\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa01e2b-dc8a-49ae-a19b-8c42bd0f2e57",
   "metadata": {},
   "source": [
    "In this section you will define a research question and perform a preliminary Exploratory Data Analysis (EDA) to address - or better, start addressing - the question at hand. This exercise will be done along the lines of the analysis done by our own Quentin Gallea in \"*A recipe to empirically answer any question quickly*\" ([Towards Data Science, 2022](https://towardsdatascience.com/a-recipe-to-empirically-answer-any-question-quickly-22e48c867dd5)). In this article, Quentin shows the first steps of an EDA that aims to explore whether heat waves have pushed governments to implement regulations against climate change (causal link). The logic is that, as it gets hotter and hotter, governments become more aware of climate change, and the problems it can cause to society, and start addressing it. In Quentin's analysis, heat waves (proxied by temperature) is the \"main explanatory variable\", rainfall is the \"explanatory variable for heterogeneity\", and regulations against climate change (proxied by the Environmental Policy Stringency Index) is the \"outcome variable\". He finds that indeed countries with relatively high temperatures have implemented more regulations against climate change. This is true especially when rainfall levels are low, as when it does not rain the damage of extreme heat is more evident to legislators, who therefore apply stricter regulations against these phenomenons.\n",
    "<br>\n",
    "<br>\n",
    "In this exercise, you will be asked to do a similar analysis on a research question of your choice, using at least two of the variables of the dataset we have created in the former questions (QOG + Polity). For example, \"what is the average temperature in 2010?\" is not a valid research question (univariate), while \"what is the impact of high temperatures on the stringency of climate regulations?\" is a valid research question (at least bivariate). As before, we will ask you some (this time more general and open) questions, and you should report your answer in the cells below each question. Use a mix of markdown and code cells to answer (markdown for text and code for graphs and tables). We should be able to run all the graphs, i.e. screenshots of graphs are not accepted. Note that for now we have put only one markdown cell and one code cell for the answer, but feel free to add as many cells as you need.\n",
    "<br>\n",
    "Beyond the python code, we will grade the interpretations of the results and the coding decision you make.\n",
    "<br>\n",
    "<br>\n",
    "Let your creativity guide you and let's have some fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f97f39-3b6d-4546-8871-b136d122854c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 9: Selecting the ingredients (how I select the variables) <a class=\"anchor\" id=\"question9\"></a>\n",
    "We have saved the clean merged data that resulted from the previous questions in \"clean_data_prepared_EDA\" (it should be the same of the one you saved in \"clean_data\"). Import the clean merged data from \"clean_data_prepared_EDA\" using this [link](https://raw.githubusercontent.com/edoardochiarotti/class_datascience/main/Notebooks/Assignment/individual_assignment/clean_data_prepared_EDA/df_qog_polity_merged.csv). Explore the variables in the newly obtained dataframe by checking the documentation of QOG and Polity. Then, define a research question that addresses a causal link between at least two of these variables. Describe the research question, why you are addressing it and the variables of interest (outcome variable, main explanatory variable and explanatory variable for heterogeneity). **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df577a5-f0f1-43e8-aeb1-0708fe53892b",
   "metadata": {},
   "source": [
    "Answer 9:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389bb830",
   "metadata": {},
   "source": [
    "#### Climate Responsibility\n",
    "\n",
    "Going back in time, from the start of the industrial revolution to now, highly industrialized countries have benefited greatly from polluting without consequence. High emission industries has created powerful nations with huge economies.  So, if we aggregate total CO2 emission per capita per country, do the high polluters take responsibility for their historic and ongoing climate impact? I want to investigate the relationship between total CO2 emission per capita and degree of environmental action. Are the high-polluting countries paying the price or are they still exploiting their influence and power to further enhance their dominance?\n",
    "\n",
    "Furthermore, one could argue that a country’s motivation to incorporate environment legislation comes from selfishness. Let me clarify with an imaginative example: Let's say a highly industrialized successful country faces a lot of climate threats such as extreme weather, wildfires, and flooding. Then it is in the country’s best interest to drive environmental politics. I would in this regard call this national climate responsibility, but not global per se. So to investigate this problem I will look at how the Climate Change Issue within each country effects the plot. The Climate Change Issue addresses to which extent each country faces climate change challanges.\n",
    "\n",
    "As this is a personal question of mine, I’m very curious to see if I can find any correlation, causality, and or discover other underlying driving factors. I initially wanted to use the “Environmental Policy Performance Index” but as the country availability is almost limited to the the western countries, and to withstand the use of the same variable as Quentin I will use the “Climate change related tax revenue given in % of GDP\" as my outcome variable. I find this suiting as it in this case directly translates to “paying for wrongdoing”.\n",
    "\n",
    "So, to sum up:\n",
    "\n",
    "- Outcome: Climate change related tax revenue (% of GDP) (`oecd_cctr_gdp`)[^1]\n",
    "- Explanatory variable: CO2 emissions per capita (`edgar_co2pc`)[^2]\n",
    "- Additional explanatory variable: Climate Change Issue Category (`epi_cch`)[^3]\n",
    "\n",
    "[^1]: Organisation for Economic Co-operation and Development (OECD). 2020. Policy Instruments for the Environment (PINE). url: oe.cd/pine\n",
    "\n",
    "[^2]: Schiller, Christof, Thorsten Hellmann, and Pia Paulini. 2020. “Sustainable Governance Indi-cators 2020”. Bertelsmann Stiftung. url: https://www.sgi-network.org/2020/Downloads\n",
    "\n",
    "[^3]: Wendling, Z.A. et al. 2020. “2020 Environmental Performance Index”. New Haven, CT: Yale Center for Environmental Law and Policy. url: https://epi.envirocenter.yale.edu/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b5531-0329-4817-a341-d1a1bb96ef2d",
   "metadata": {},
   "source": [
    "### Question 10: Picking the right quantity of each ingredient (how I select my sample) <a class=\"anchor\" id=\"question10\"></a>\n",
    "Explore the data availability of your variables of interest and select a clean sample for the analysis. Describe this sample with the help of summary-statistics tables and maps. **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef40ba-ff3a-4983-99c7-151be2f9622e",
   "metadata": {},
   "source": [
    "Answer 10:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb7d8fb",
   "metadata": {},
   "source": [
    "The table below reveals that we have 180 observations for `epi_cch` - which is consistent with what the documentation said. It's an assessment of 8 indicators done in 2019 measuring climate challange. The score varies from 0 to 100. As this i a new variable, it may not express the historic climate challange a country faces. But as climate challanges are somewhat consistant i think that a country facing a lot of threats in 2019 may also have been so historically. So i will continue with this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b575ca1f-de5a-4b03-a51a-7f0cc5fa18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/edoardochiarotti/class_datascience/main/Notebooks/Assignment/individual_assignment/clean_data_prepared_EDA/df_qog_polity_merged.csv'\n",
    "df_qog_polity_merged = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb262470-32d1-4d2c-af1c-8616daed69e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>oecd_cctr_gdp</th>\n",
       "      <th>edgar_co2pc</th>\n",
       "      <th>epi_cch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15150.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>8065.000000</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1983.000000</td>\n",
       "      <td>76.894252</td>\n",
       "      <td>4.774815</td>\n",
       "      <td>49.758889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21.649425</td>\n",
       "      <td>31.219362</td>\n",
       "      <td>9.402640</td>\n",
       "      <td>17.056272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1946.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013578</td>\n",
       "      <td>12.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1964.000000</td>\n",
       "      <td>73.792000</td>\n",
       "      <td>0.378785</td>\n",
       "      <td>36.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1983.000000</td>\n",
       "      <td>89.542000</td>\n",
       "      <td>1.643606</td>\n",
       "      <td>50.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2002.000000</td>\n",
       "      <td>99.097500</td>\n",
       "      <td>6.241199</td>\n",
       "      <td>63.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>108.098000</td>\n",
       "      <td>173.602320</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               year  oecd_cctr_gdp  edgar_co2pc     epi_cch\n",
       "count  15150.000000    2099.000000  8065.000000  180.000000\n",
       "mean    1983.000000      76.894252     4.774815   49.758889\n",
       "std       21.649425      31.219362     9.402640   17.056272\n",
       "min     1946.000000       0.000000     0.013578   12.100000\n",
       "25%     1964.000000      73.792000     0.378785   36.850000\n",
       "50%     1983.000000      89.542000     1.643606   50.850000\n",
       "75%     2002.000000      99.097500     6.241199   63.150000\n",
       "max     2020.000000     108.098000   173.602320   95.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer 10:\n",
    "# Subsetting and picking the variables i want to investigate\n",
    "df_subset = df_qog_polity_merged[['country', 'iso3', 'year', 'oecd_cctr_gdp', 'edgar_co2pc', 'epi_cch']]\n",
    "df_subset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd0172c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>iso3</th>\n",
       "      <th>year</th>\n",
       "      <th>oecd_cctr_gdp</th>\n",
       "      <th>edgar_co2pc</th>\n",
       "      <th>epi_cch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.295597</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.925957</td>\n",
       "      <td>56.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZA</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.230883</td>\n",
       "      <td>52.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>Angola</td>\n",
       "      <td>AGO</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.812239</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>ATG</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.899879</td>\n",
       "      <td>58.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14382</th>\n",
       "      <td>Uruguay</td>\n",
       "      <td>URY</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.885312</td>\n",
       "      <td>55.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14457</th>\n",
       "      <td>Uzbekistan</td>\n",
       "      <td>UZB</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.895439</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14532</th>\n",
       "      <td>Venezuela, Bolivarian Republic of</td>\n",
       "      <td>VEN</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.357467</td>\n",
       "      <td>63.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14607</th>\n",
       "      <td>Samoa</td>\n",
       "      <td>WSM</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.699500</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14907</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>ZMB</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.414457</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 country iso3  year  oecd_cctr_gdp  \\\n",
       "73                           Afghanistan  AFG  2019            NaN   \n",
       "148                              Albania  ALB  2019            NaN   \n",
       "223                              Algeria  DZA  2019            NaN   \n",
       "373                               Angola  AGO  2019            NaN   \n",
       "448                  Antigua and Barbuda  ATG  2019            NaN   \n",
       "...                                  ...  ...   ...            ...   \n",
       "14382                            Uruguay  URY  2019            NaN   \n",
       "14457                         Uzbekistan  UZB  2019            NaN   \n",
       "14532  Venezuela, Bolivarian Republic of  VEN  2019            NaN   \n",
       "14607                              Samoa  WSM  2019            NaN   \n",
       "14907                             Zambia  ZMB  2019            NaN   \n",
       "\n",
       "       edgar_co2pc  epi_cch  \n",
       "73        0.295597     22.2  \n",
       "148       1.925957     56.8  \n",
       "223       4.230883     52.5  \n",
       "373       0.812239     49.0  \n",
       "448       4.899879     58.5  \n",
       "...            ...      ...  \n",
       "14382     1.885312     55.5  \n",
       "14457     2.895439     65.9  \n",
       "14532     3.357467     63.3  \n",
       "14607     0.699500     35.0  \n",
       "14907     0.414457     26.5  \n",
       "\n",
       "[180 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we see all notna values for epi_cch - consistent with the documentation - all from 2019.\n",
    "df_subset[df_subset.epi_cch.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6012f",
   "metadata": {},
   "source": [
    "Now lets see how dense our `Tax%GDP` data is. Between 2000 and 2017 we have mean above 80 - meaning we have Carbon Tax % of GDP for over 80 countries.This is quite good. I will use this time interval in my studies. Since climate change and its reactive measures did not get any widespread attention before Al Gore addressed GHG and its alarming warming effects in early 1990s, i will also neglect the effects on not using any data prior to 2000. Im making an assumption that the  climate change tax as a percentage of GDP in % was relatively small prior to 1990s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "596be62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1999</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2001</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2002</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2003</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2004</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2005</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2006</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2007</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2008</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2009</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2010</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2011</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2012</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2013</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2014</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2015</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  counts\n",
       "0   1994      61\n",
       "1   1995      69\n",
       "2   1996      70\n",
       "3   1997      70\n",
       "4   1998      73\n",
       "5   1999      73\n",
       "6   2000      81\n",
       "7   2001      83\n",
       "8   2002      83\n",
       "9   2003      83\n",
       "10  2004      86\n",
       "11  2005      89\n",
       "12  2006      93\n",
       "13  2007      93\n",
       "14  2008      96\n",
       "15  2009      95\n",
       "16  2010      96\n",
       "17  2011      95\n",
       "18  2012      95\n",
       "19  2013      96\n",
       "20  2014      97\n",
       "21  2015      91\n",
       "22  2016      90\n",
       "23  2017      86\n",
       "24  2018      55"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset[pd.notnull(df_subset['oecd_cctr_gdp'])].groupby(['year']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "216e870f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AFG', 'ALB', 'DZA', 'AGO', 'ATG', 'AZE', 'ARG', 'AUS', 'AUT',\n",
       "       'BHS', 'BHR', 'BGD', 'ARM', 'BRB', 'BEL', 'BTN', 'BOL', 'BIH',\n",
       "       'BWA', 'BRA', 'BLZ', 'SLB', 'BRN', 'BGR', 'MMR', 'BDI', 'BLR',\n",
       "       'KHM', 'CMR', 'CAN', 'CPV', 'CAF', 'LKA', 'TCD', 'CHL', 'CHN',\n",
       "       'TWN', 'COL', 'COM', 'COG', 'COD', 'CRI', 'HRV', 'CUB', 'CYP',\n",
       "       'CZE', 'BEN', 'DNK', 'DMA', 'DOM', 'ECU', 'SLV', 'GNQ', 'ETH',\n",
       "       'ERI', 'EST', 'FJI', 'FIN', 'FRA', 'DJI', 'GAB', 'GEO', 'GMB',\n",
       "       'DEU', 'GHA', 'KIR', 'GRC', 'GRD', 'GTM', 'GIN', 'GUY', 'HTI',\n",
       "       'HND', 'HUN', 'ISL', 'IND', 'IDN', 'IRN', 'IRQ', 'IRL', 'ISR',\n",
       "       'ITA', 'CIV', 'JAM', 'JPN', 'KAZ', 'JOR', 'KEN', 'KOR', 'KWT',\n",
       "       'KGZ', 'LAO', 'LBN', 'LSO', 'LVA', 'LBR', 'LTU', 'LUX', 'MDG',\n",
       "       'MWI', 'MYS', 'MDV', 'MLI', 'MLT', 'MRT', 'MUS', 'MEX', 'MNG',\n",
       "       'MDA', 'MNE', 'MAR', 'MOZ', 'OMN', 'NAM', 'NPL', 'NLD', 'VUT',\n",
       "       'NZL', 'NIC', 'NER', 'NGA', 'NOR', 'FSM', 'MHL', 'PAK', 'PAN',\n",
       "       'PNG', 'PRY', 'PER', 'PHL', 'POL', 'PRT', 'GNB', 'TLS', 'QAT',\n",
       "       'ROU', 'RUS', 'RWA', 'LCA', 'VCT', 'STP', 'SAU', 'SEN', 'SRB',\n",
       "       'SYC', 'SLE', 'SGP', 'SVK', 'VNM', 'SVN', 'ZAF', 'ZWE', 'ESP',\n",
       "       'SDN', 'SUR', 'SWZ', 'SWE', 'CHE', 'TJK', 'THA', 'TGO', 'TON',\n",
       "       'TTO', 'ARE', 'TUN', 'TUR', 'TKM', 'UGA', 'UKR', 'MKD', 'EGY',\n",
       "       'GBR', 'TZA', 'USA', 'BFA', 'URY', 'UZB', 'VEN', 'WSM', 'ZMB'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But since we only have data for epi_cch from 2019. I will take this data out of the dataframe, remove all NaNs and then we can merge later on when we have aggregated the other datasets.\n",
    "\n",
    "climate_challange_df = df_subset[['iso3', 'year' , 'country', 'epi_cch']]\n",
    "\n",
    "climate_challange_clean = climate_challange_df.dropna()\n",
    "\n",
    "climate_challage_iso3_codes = climate_challange_clean.iso3.unique()\n",
    "\n",
    "# These are the countries that has a value for epi_cch (climate challange issue) from 2019 and the countries we will work with.\n",
    "climate_challage_iso3_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1c3e995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       NaN\n",
       "1       NaN\n",
       "2       NaN\n",
       "3       NaN\n",
       "4       NaN\n",
       "         ..\n",
       "15145   NaN\n",
       "15146   NaN\n",
       "15147   NaN\n",
       "15148   NaN\n",
       "15149   NaN\n",
       "Name: epi_cch, Length: 15150, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And for the time being we can pop the epi_cch column from our QoG_polity_subset\n",
    "df_subset.pop('epi_cch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433aa29a",
   "metadata": {},
   "source": [
    "Here is a table for the CO2 emission per capita for each country. The first recordnings are from 1970. Which can make my hypothesis a bit hard to answer, as emissions before 1970 are not in this dataset. I tried looking online to see any datasets to merge in, and use. I found a reliable source at https://ourworldindata.org/co2-and-other-greenhouse-gas-emissions. My plan now is to use these values to give a more historicly correct picture. The CO2 data has a corresponding GitHub page here: https://github.com/owid/co2-data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "825ac260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1971</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1972</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1973</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1974</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1975</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1976</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1977</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1978</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1979</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1980</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1981</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1982</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1983</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1984</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1985</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1986</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1987</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1988</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1989</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1990</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1991</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1992</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1993</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1994</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1995</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1996</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1997</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1998</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1999</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2000</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2001</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2002</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2003</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2004</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2005</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2006</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2007</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2008</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2009</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2010</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2011</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2012</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2013</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2014</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2015</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2016</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2017</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2018</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2019</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  counts\n",
       "0   1970     124\n",
       "1   1971     126\n",
       "2   1972     129\n",
       "3   1973     129\n",
       "4   1974     131\n",
       "5   1975     133\n",
       "6   1976     139\n",
       "7   1977     141\n",
       "8   1978     141\n",
       "9   1979     145\n",
       "10  1980     146\n",
       "11  1981     147\n",
       "12  1982     148\n",
       "13  1983     148\n",
       "14  1984     150\n",
       "15  1985     150\n",
       "16  1986     150\n",
       "17  1987     150\n",
       "18  1988     150\n",
       "19  1989     150\n",
       "20  1990     152\n",
       "21  1991     153\n",
       "22  1992     171\n",
       "23  1993     175\n",
       "24  1994     175\n",
       "25  1995     176\n",
       "26  1996     176\n",
       "27  1997     176\n",
       "28  1998     176\n",
       "29  1999     176\n",
       "30  2000     176\n",
       "31  2001     176\n",
       "32  2002     177\n",
       "33  2003     177\n",
       "34  2004     177\n",
       "35  2005     177\n",
       "36  2006     176\n",
       "37  2007     176\n",
       "38  2008     176\n",
       "39  2009     176\n",
       "40  2010     176\n",
       "41  2011     176\n",
       "42  2012     177\n",
       "43  2013     177\n",
       "44  2014     177\n",
       "45  2015     177\n",
       "46  2016     177\n",
       "47  2017     177\n",
       "48  2018     177\n",
       "49  2019     177"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset[pd.notnull(df_subset['edgar_co2pc'])].groupby(['year']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c13ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching data from Our World In Data.\n",
    "# This set is one of the most comprehensive data collections of total historically CO2 emissions.\n",
    "url = 'https://nyc3.digitaloceanspaces.com/owid-public/data/co2/owid-co2-data.csv'\n",
    "df_co2 = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b0148",
   "metadata": {},
   "source": [
    "Doing some housekeeping (subsetting a useable dataframe and changing iso_code to iso3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "762f13ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19300"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_co2_subset = df_co2[['country', 'year', 'iso_code', 'population', 'co2']].copy()\n",
    "\n",
    "df_co2_subset.rename(columns={'iso_code': 'iso3'}, inplace=True)\n",
    "\n",
    "# Extracting the countries we have from climate_challage_iso3_codes.\n",
    "df_co2_subset_country_limited = df_co2_subset[df_co2_subset.iso3.isin(climate_challage_iso3_codes)].copy()\n",
    "len(df_co2_subset_country_limited)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52ee10bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>iso3</th>\n",
       "      <th>population</th>\n",
       "      <th>co2</th>\n",
       "      <th>co2_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1949</td>\n",
       "      <td>AFG</td>\n",
       "      <td>7624058.0</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.001967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1950</td>\n",
       "      <td>AFG</td>\n",
       "      <td>7752117.0</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.010836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1951</td>\n",
       "      <td>AFG</td>\n",
       "      <td>7840151.0</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.011734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1952</td>\n",
       "      <td>AFG</td>\n",
       "      <td>7935996.0</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.011593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1953</td>\n",
       "      <td>AFG</td>\n",
       "      <td>8039684.0</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.013185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26003</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2016</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>14030338.0</td>\n",
       "      <td>10.738</td>\n",
       "      <td>0.765342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26004</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2017</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>14236599.0</td>\n",
       "      <td>9.582</td>\n",
       "      <td>0.673054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26005</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>14438812.0</td>\n",
       "      <td>11.854</td>\n",
       "      <td>0.820982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26006</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2019</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>14645473.0</td>\n",
       "      <td>10.949</td>\n",
       "      <td>0.747603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26007</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>10.531</td>\n",
       "      <td>0.708541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19300 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           country  year iso3  population     co2  co2_per_capita\n",
       "0      Afghanistan  1949  AFG   7624058.0   0.015        0.001967\n",
       "1      Afghanistan  1950  AFG   7752117.0   0.084        0.010836\n",
       "2      Afghanistan  1951  AFG   7840151.0   0.092        0.011734\n",
       "3      Afghanistan  1952  AFG   7935996.0   0.092        0.011593\n",
       "4      Afghanistan  1953  AFG   8039684.0   0.106        0.013185\n",
       "...            ...   ...  ...         ...     ...             ...\n",
       "26003     Zimbabwe  2016  ZWE  14030338.0  10.738        0.765342\n",
       "26004     Zimbabwe  2017  ZWE  14236599.0   9.582        0.673054\n",
       "26005     Zimbabwe  2018  ZWE  14438812.0  11.854        0.820982\n",
       "26006     Zimbabwe  2019  ZWE  14645473.0  10.949        0.747603\n",
       "26007     Zimbabwe  2020  ZWE  14862927.0  10.531        0.708541\n",
       "\n",
       "[19300 rows x 6 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding a column with co2 per capita and converting the scale so we have co2_per_capita in tons\n",
    "df_co2_subset_country_limited['co2_per_capita'] =  df_co2_subset_country_limited['co2']/df_co2_subset_country_limited['population']*1000000\n",
    "\n",
    "df_co2_subset_country_limited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cd450",
   "metadata": {},
   "source": [
    "Now i think we have a good working foundation for our emission data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec73305",
   "metadata": {},
   "source": [
    "Lets work further with our qog_polity dataset. I want to extract the data from 2000-2018. And then sum up the mean Climate change related tax revenue (% of GDP) per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d95f5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_2000_2018 = df_subset[(df_subset.year >= 2000) & (df_subset.year <= 2018)]\n",
    "\n",
    "# Dropping the lackful emission column\n",
    "df_subset_2000_2018_slim = df_subset_2000_2018.drop(columns='edgar_co2pc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e5ae083e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>oecd_cctr_gdp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3420.000000</td>\n",
       "      <td>1683.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2009.000000</td>\n",
       "      <td>76.551483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.478027</td>\n",
       "      <td>30.878763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2004.000000</td>\n",
       "      <td>72.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2009.000000</td>\n",
       "      <td>88.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2014.000000</td>\n",
       "      <td>98.796500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>108.098000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              year  oecd_cctr_gdp\n",
       "count  3420.000000    1683.000000\n",
       "mean   2009.000000      76.551483\n",
       "std       5.478027      30.878763\n",
       "min    2000.000000       0.000000\n",
       "25%    2004.000000      72.316000\n",
       "50%    2009.000000      88.748000\n",
       "75%    2014.000000      98.796500\n",
       "max    2018.000000     108.098000"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets also here limit the countries to the ones we found in climate_challage_iso3_codes\n",
    "\n",
    "df_qog_pol_clean = df_subset_2000_2018_slim[df_subset_2000_2018_slim.iso3.isin(climate_challage_iso3_codes)].copy()\n",
    "df_qog_pol_clean.describe()\n",
    "\n",
    "# Wait .... hmmm - the mean for Tax to climate change as in % of GDP is 76%?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76faae83",
   "metadata": {},
   "source": [
    "\n",
    "#### A mistake?\n",
    "...Wait a minute, how can the % of GDP from Climate Change tax be over 100% and have an mean of 76%? This seems off. I will check the definition for the variable once again.\n",
    "\n",
    "> Climate change-related tax revenue as a percentage of gross domestic product (GDP). Includes\n",
    "taxes, fees and charges, tradable permits, deposit-refund systems, subsidies, and voluntary ap-\n",
    "proaches related to the domain of climate change.\n",
    "\n",
    "\n",
    "In my view this should probably be lower...\n",
    "\n",
    "I will go to the data source ( http://oe.cd/pine) and see if the values there are the same. I found this https://www.compareyourcountry.org/environmental-taxes/en/1/183/default  a simple graph showing that the numbers in the qog_pol dataset is wrong (or at least its something odd with the)\n",
    "\n",
    "Then i found this site where i could download the data i wanted. At OECD (https://stats.oecd.org/Index.aspx?DataSetCode=ERTR#). I downloaded it as i couldn't find any other places where this data was accesssable by url or thourgh an api.\n",
    "\n",
    "##### Its 80mb...\n",
    "\n",
    "\n",
    "So i did some adjustments to the file before commiting it to my public `data_folder` in the `class_datacience` repo on GitHub.\n",
    "At first i downloaded the whole file locally. And investigated what could be removed. I found out that there were categories for the environmental tax. All tax bases', 'Energy', 'Transport', 'Pollution', 'Resources. I'm interested in all tax bases, so that should shrank the data file a lot.\n",
    "\n",
    "I'm leaving the code i used below, but commented out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "400ee2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the original dataset that QoG used, its huge...\n",
    "climate_tax = pd.read_csv('data/ERTR_29102022172652539.csv', low_memory=False)\n",
    "\n",
    "# Checking the keys\n",
    "#climate_tax.Category.unique()\n",
    "\n",
    "# Narrowing\n",
    "climate_tax_total = climate_tax[climate_tax.Category == 'All tax bases']\n",
    "\n",
    "# Exporting\n",
    "climate_tax_total.to_csv('oecd_climate_tax_total.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6d1742",
   "metadata": {},
   "source": [
    "That shrunk the filesize to 15mb, which is better. Ill uploead that on my public data_science repo, and then we can continue to invest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5e19011e",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/pcw/Documents/GitHub/class_datascience/Notebooks/Assignment/individual_assignment/01_individual_assignment.ipynb Cell 117\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pcw/Documents/GitHub/class_datascience/Notebooks/Assignment/individual_assignment/01_individual_assignment.ipynb#Y220sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://raw.githubusercontent.com/percw/class_datascience/main/Notebooks/Assignment/individual_assignment/data/oecd_climate_tax_total.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pcw/Documents/GitHub/class_datascience/Notebooks/Assignment/individual_assignment/01_individual_assignment.ipynb#Y220sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m climate_tax \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(url, low_memory\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m     f,\n\u001b[1;32m   1220\u001b[0m     mode,\n\u001b[1;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1227\u001b[0m )\n\u001b[1;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:667\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    664\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    666\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[0;32m--> 667\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    668\u001b[0m     path_or_buf,\n\u001b[1;32m    669\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    670\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    671\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    672\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    673\u001b[0m )\n\u001b[1;32m    675\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    676\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:336\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    335\u001b[0m req_info \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[39m=\u001b[39mstorage_options)\n\u001b[0;32m--> 336\u001b[0m \u001b[39mwith\u001b[39;00m urlopen(req_info) \u001b[39mas\u001b[39;00m req:\n\u001b[1;32m    337\u001b[0m     content_encoding \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mContent-Encoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m content_encoding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    339\u001b[0m         \u001b[39m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:236\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39mthe stdlib.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m \u001b[39mreturn\u001b[39;00m urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlopen(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/urllib/request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mfor\u001b[39;00m processor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_response\u001b[39m.\u001b[39mget(protocol, []):\n\u001b[1;32m    522\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 523\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[1;32m    525\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/urllib/request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[39m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[39m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[1;32m    633\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[1;32m    635\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/urllib/request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m http_err:\n\u001b[1;32m    560\u001b[0m     args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp_error_default\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m orig_args\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    493\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/urllib/request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_error_default\u001b[39m(\u001b[39mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 641\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(req\u001b[39m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/percw/class_datascience/main/Notebooks/Assignment/individual_assignment/data/oecd_climate_tax_total.csv'\n",
    "climate_tax = pd.read_csv(url, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837df33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a lot of combinations countrycode-year combinations. \n",
    "more_than_one_combination(climate_tax, 'COU', 'Year', 'Value')\n",
    "\n",
    "# Here we can see what the keys looks like\n",
    "climate_tax.columns\n",
    "\n",
    "# I will check one country to start with to see if i can see that all of these values are about\n",
    "switzerland = climate_tax[climate_tax.Country == 'Switzerland']\n",
    "switzerland\n",
    "\n",
    "# Looks like there are different categories. I want Climate change. Lets filter on that\n",
    "switzerland_cc = switzerland[switzerland['Environmental domain'] == 'Climate change']\n",
    "switzerland_cc\n",
    "\n",
    "# And the variable im looking for is Tax revenue, % of GDP\n",
    "switzerland_cc_tax = switzerland_cc[switzerland_cc['Variable'] == 'Tax revenue, % of GDP']\n",
    "\n",
    "more_than_one_combination(switzerland_cc_tax, 'COU', 'Year', 'Value')\n",
    "# After this we dont have any ambiguity in our country-year combination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac8024",
   "metadata": {},
   "source": [
    "I have to look at the defininton in the codebook given by QoG again.\n",
    "\n",
    "> Climate change-related tax revenue as a percentage of gross domestic product (GDP). Includes\n",
    "taxes, fees and charges, tradable permits, deposit-refund systems, subsidies, and voluntary ap-\n",
    "proaches related to the domain of climate change.\n",
    "\n",
    "It looks like QoG databse used the `Tax revenue, % of total environmentally related tax revenue` value instead of `Tax revenue, % of GDP`. This value is much higer as it is the percentage of Tax related to climate change OVER environmental related tax revenue. Now it makes sense. Lets try to extract the correct values. I may be wrong here, overlooked something or misintepreted the definition. But if not - i may have found an error in the QoG database.\n",
    "\n",
    "So - to find the value that I want i need to define my subset with a couple of constraints.\n",
    "\n",
    "- Environmental domain : Climate change\n",
    "- Unit : Percentage\n",
    "- Variable : Tax revenue, % of GDP\n",
    "- CAT : TOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9685621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "climate_tax = climate_tax[(climate_tax['Environmental domain'] == 'Climate change') & (climate_tax['Unit'] == 'Percentage') & (climate_tax['Variable'] == 'Tax revenue, % of GDP') & (climate_tax['CAT'] == 'TOT')]\n",
    "\n",
    "# Great, it worked for the whole dataset as well.\n",
    "more_than_one_combination(climate_tax, 'COU', 'Year', 'Value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can start exploring the new dataset and extracting what we want\n",
    "\n",
    "ct_df = climate_tax[['COU', 'Year', 'Country', 'Value']]\n",
    "ct_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f552778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having - % seems odd. Lets look a bit further\n",
    "\n",
    "ct_df[ct_df.Value == -1.540000]\n",
    "\n",
    "# Mexico seems to have several - values.\n",
    "ct_df[ct_df.Country == 'Mexico']\n",
    "\n",
    "# Lets see if any other countries have that as well\n",
    "ct_df[ct_df.Value < 0]\n",
    "\n",
    "# Its only mexico. I dont think these data are accurate so i will remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d56b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_indexes = ct_df[ct_df.Country == 'Mexico'].index\n",
    "\n",
    "ct_df_clean = ct_df.drop(index=mexico_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602992e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now i want to rename the columns to match the QoG data\n",
    "ct_df_clean.rename(columns={'COU' : 'iso3', 'Year': 'year', 'Country' : 'country', 'Value' : 'oecd_cctr_gdp'}, inplace=True)\n",
    "ct_df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7383d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now i want to see how dense my data is\n",
    "\n",
    "ct_df_clean[pd.notnull(ct_df_clean['oecd_cctr_gdp'])].groupby(['year']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should see if there are any countries with very few tax registrations\n",
    "df_oecd_cctr_gdp_count = ct_df_clean[pd.notnull(ct_df_clean['oecd_cctr_gdp'])].groupby(['country']).size().reset_index(name=\"counts\")\n",
    "df_oecd_cctr_gdp_count\n",
    "\n",
    "# These countries has less than 15 observations, i will remove thme \n",
    "countries = df_oecd_cctr_gdp_count[df_oecd_cctr_gdp_count.counts < 15].country.unique()\n",
    "\n",
    "for c in countries:\n",
    "\tc_index = ct_df_clean[ct_df_clean.country == c].index\n",
    "\tct_df_clean = ct_df_clean.drop(index=c_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5bbfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also make the oecd tax data sub subset from 2000 to 2018\n",
    "climate_tax_df = ct_df_clean[(ct_df_clean.year >= 2000) & (ct_df_clean.year <= 2018)]\n",
    "climate_tax_df\n",
    "\n",
    "# And lets make a better name for the QOG_Polity df\n",
    "qog_pol_df = df_subset_2000_2018_slim\n",
    "qog_pol_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41b2a9",
   "metadata": {},
   "source": [
    "#### Lets aggregate\n",
    "\n",
    "So we now have 3 different datasets.\n",
    "\n",
    "- climate_tax_df : range 2000-2018. Climate Change tax % of GDP\n",
    "- climate_challange_clean : 2019. Score of Climate Challange a cnountry faces\n",
    "- df_co2 : range 1750-2020. Annual Emission per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0910ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_mean = climate_tax_df.groupby(['iso3', 'country'])['oecd_cctr_gdp'].mean().reset_index(name='mean_oecd_cctr_gdp')\n",
    "co2_total = df_co2.groupby(['iso3', 'country'])['co2'].sum().reset_index(name='total_co2')\n",
    "\n",
    "tax_mean\n",
    "\n",
    "merger1 = pd.merge(left=tax_mean, right=co2_total, on=['iso3', 'country'], how='left', validate='1:1')\n",
    "\n",
    "merger2 = pd.merge(left=merger1, right=climate_challange_clean, on=['iso3', 'country'], how='left', validate='1:1')\n",
    "\n",
    "# Now i finally got all the values i wanted\n",
    "merger2.pop('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merger2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4434c1-4ca5-44af-a24c-67a1b8d20cc6",
   "metadata": {},
   "source": [
    "### Question 11: Tasting and preparing the ingredients (univariate analysis) <a class=\"anchor\" id=\"question11\"></a>\n",
    "Do an univariate analysis for each variable you have chosen (outcome variable, main explanatory variable and explanatory variable for heterogeneity):\n",
    "- Prepare the variable, for example see if you need to transform the data further, i.e. log-transform, define a categorical variable, deal with outliers, etc.\n",
    "- Understand the nature of the variable, i.e. continuous, categorical, binary, etc., which then allows to pick the right statistical tool in the bivariate analysis.\n",
    "- Get an idea of the variable's behaviour across time and space.\n",
    "\n",
    "Describe these steps and the conclusions you can draw with the help of histograms, tables, maps and line graphs. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebcafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_obs_per_country = df_cleaned.groupby(['iso3'])['polity2'].size().reset_index()\n",
    "pol_obs_map = px.choropleth(pol_obs_per_country, locations='iso3',\n",
    "                    title='Number of polity2 observations by country',\n",
    "                    locationmode='ISO-3',\n",
    "                    color='polity2', \n",
    "                    hover_name='iso3',\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma)\n",
    "\n",
    "pol_obs_map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef687c9b-69c5-49b0-adb3-2aac7406112c",
   "metadata": {},
   "source": [
    "Answer 11:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fffac2-d33c-410f-9f10-c190a4c59641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 11:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1ad4a-70fc-4f39-bffb-cf10dd93205f",
   "metadata": {},
   "source": [
    "### Question 12: Cooking the ingredients together (bivariate analysis) <a class=\"anchor\" id=\"question12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a0c658-8c22-4bb0-b1ef-4b5860f6cc31",
   "metadata": {},
   "source": [
    "Considering the \"nature\" of your variables (continuous, categorical, binary, etc.), pick the right tool / tools for a preliminary bivariate analysis, i.e. correlation tables, bar/line graphs, scatter plots, etc. Use these tools to describe your preliminary bivariate analysis and your findings. **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8bb04c-9223-4e3d-b815-2d7ee3657f00",
   "metadata": {},
   "source": [
    "Answer 12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c0962-c661-4649-b8f3-e721e7ef66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 12:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0a05d-004a-42f3-b472-b7a808cd0dc1",
   "metadata": {},
   "source": [
    "### Question 13: Tasting the new recipe (conclusion) <a class=\"anchor\" id=\"question13\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b861d-d4d5-42b1-94ad-5aa33f9bf7a2",
   "metadata": {},
   "source": [
    "Explain what you learned, the problem faced, what would you do next (you can suggest other data you would like to have etc). **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5536aef2-a8df-4e17-9089-01ba8d93c029",
   "metadata": {},
   "source": [
    "Answer 13:\n",
    "\n",
    "FOund out that the countries do pay some tax - but is this tax money used to further reduce its impact?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d998423",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c967b75bd3da4530419091f2a82585a1448630d30358f0f328fd8248d7345ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
