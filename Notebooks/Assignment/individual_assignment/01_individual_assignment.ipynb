{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795860f2-eabf-40d1-954c-843dcf672a7d",
   "metadata": {},
   "source": [
    "# MGT-499 Statistics and Data Science - Individual Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58faf4-db28-41c1-9e6d-ead96d328eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import here what you need\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5761d-3238-4e69-951f-6fea4557ef41",
   "metadata": {},
   "source": [
    "This notebook contains the individual assignment for the class MGT-499 Statistics and Data Science. Important information:\n",
    "- **Content**: the assignment is divided in two main parts, namely data cleaning (2 datasets) and Exploratory Data Analysis, for a total of 13 main questions (see table of contents). Some of these main questions are divided in sub questions. In the first part, the questions are very specific, while in the second part they are more open.\n",
    "- **Deadline**: Tuesday 8th of November at 23:59. \n",
    "- **Final Output**: a Jupyter notebook, which we (teachers) can run. \n",
    "- **Answering the Questions**: you will find the questions in markdown cells below. Under each of these cells, you will find a cell / cells for answers. Type there your answer. For the answer to be correct, the cell with the answer must run without error (unless specified). You can use markdown cells for the answers that require text.\n",
    "- **Submission**: submit the assignment on Moodle, under [Individual Assignment](https://moodle.epfl.ch/mod/assign/view.php?id=1222846)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e43ccf-9ed6-44ea-8dd3-184cb9e7c499",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Content\n",
    "- [Polity5 Dataset](#polity5)  \n",
    "    - [Question 1: Import the data and get a first glance](#question1)\n",
    "    - [Question 2: Select some variables](#question2)\n",
    "    - [Question 3: Missing Values](#question3)\n",
    "    - [Question 4: Check Polity2](#question4)\n",
    "- [Quality of Government (QOG) Environmental Indicators Dataset](#qog)  \n",
    "    - [Question 5: Import the data and do few fixes](#question5)\n",
    "    - [Question 6: Merge QOG and Polity5 ... first attempt](#question6)\n",
    "    - [Question 7: Merge QOG and Polity5 ... second attempt](#question7)\n",
    "    - [Question 8: Clean the merged dataframe](#question8)\n",
    "- [Exploratory Data Analysis](#eda)\n",
    "    - [Question 9: Selecting the ingredients for the recipe (how I select the variables)](#question9)  \n",
    "    - [Question 10: Picking the right quantity of each ingredient (how I select my sample)](#question10)\n",
    "    - [Question 11: Tasting and preparing the ingredients (univariate analysis)](#question11)\n",
    "    - [Question 12: Cooking the ingredients together (bivariate analysis)](#question12)\n",
    "    - [Question 13: Tasting the new recipe (conclusion)](#question13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142f738-6239-4e12-9c65-fd27b4db73ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Polity5 data <a class=\"anchor\" id=\"polity5\"></a>\n",
    "\n",
    "Polity5 is a widely used democracy scale. The raw data as well as the codebook are available [here](http://www.systemicpeace.org/inscrdata.html). For this assignment, we have modified a bit the original version, for example we have added the iso3 code for countries to make you save time. You can find the modified version [here](https://raw.githubusercontent.com/edoardochiarotti/class_datascience/main/Notebooks/Assignment/individual_assignment/data/polity2_iso3.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903da7f-480e-4777-a4ef-9ab94b09e473",
   "metadata": {},
   "source": [
    "### Question 1: import the data and get a first glance <a class=\"anchor\" id=\"question1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f6829-6c8f-451a-9670-4d1c7a103c12",
   "metadata": {},
   "source": [
    "1a) Import the csv 'polity2_iso3.csv' (file provided in the link [here](https://raw.githubusercontent.com/edoardochiarotti/class_datascience/main/Notebooks/Assignment/individual_assignment/data/polity2_iso3.csv)) as a panda dataframe (ignore the warning message) **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e4014-e55c-4994-a633-61a8fdf1e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1a\n",
    "url = 'https://raw.githubusercontent.com/edoardochiarotti/class_datascience/main/Notebooks/Assignment/individual_assignment/data/polity2_iso3.csv'\n",
    "polity_data = pd.read_csv(url, low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd738c61-10ca-4439-8bcf-20e6987e3e5f",
   "metadata": {},
   "source": [
    "1b) Display the first 10 rows **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10544e0c-33b5-4b90-ad68-4cfe8d6c40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1b\n",
    "polity_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6decfca-afb8-4c77-881c-8fb78d96e68d",
   "metadata": {},
   "source": [
    "1c) Display the data types of all the variables included in the data **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa768541-03f9-424a-a690-d6132298f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1c\n",
    "polity_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ee31a-7cc9-44d2-b7f8-242f6ee5ebfc",
   "metadata": {},
   "source": [
    "1d) By looking at your answer in 1c, what is the difference between the different types of variables? Why the type of some variables is defined as object? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97f628-71b6-4aac-9c9c-596773557131",
   "metadata": {},
   "source": [
    "Answer 1d:\n",
    "\n",
    "We can see that we have four different dataypes: `object`, `float64` and `int64`. `Object` is  the pandas type for mixed values s.a. strings and numbers. In python the equivivalent  type is `string`. `Float64` refers to numeric characters with decimals. If columns contain numbers and NaN pandas will defualt to `float64`. And for `int64` is for integer numbers, and the 64 refers to memory allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfba84f-b897-4fcb-880d-9d8995045239",
   "metadata": {},
   "source": [
    "### Question 2. Select some variables <a class=\"anchor\" id=\"question2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9776a-26ed-4154-af73-0e2b337e9e74",
   "metadata": {},
   "source": [
    "2a) Create a subset dataframe that contains the variables 'iso3', 'country', 'year', 'polity2' and display it **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd0a39-386d-4397-93eb-8eb7e0415a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2a\n",
    "polity_data_subset = (polity_data[['iso3', 'country', 'year', 'polity2']]).copy()\n",
    "polity_data_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4883d-8f9e-4b1f-9ce4-c62267090287",
   "metadata": {},
   "source": [
    "2b) Display the type of the variable \"year\" **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc6b73-70d9-42c0-9cd1-716033325a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2b\n",
    "polity_data_subset['year'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639b2d3-b5e6-40f3-93c9-6547bcdcb814",
   "metadata": {},
   "source": [
    "2c) Convert the variable \"year\" to string **(1 point)**\n",
    "<br>\n",
    "Hint: if you get a warning message of the type \"SettingWithCopyWarning\", it is because you did not subset the data in the right way. Go back to your class notes and check the different ways to subset a dataframe, and try again. If you do it correctly, you will not get the warning message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d8f60-9875-40be-9f78-0c7c2837a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2c\n",
    "polity_data_subset['year'] = polity_data_subset['year'].astype('string')\n",
    "polity_data_subset['year'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0babf3-0e33-4e13-9289-523ecef5e004",
   "metadata": {},
   "source": [
    "### Question 3: Missing Values <a class=\"anchor\" id=\"question3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb19a0f-bd7c-4d9c-bdfa-05e55e4eb26f",
   "metadata": {},
   "source": [
    "3a) Subset the rows that have iso3 missing and display **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb48bb3-a92a-4565-8a5c-f22f642f2661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3a\n",
    "iso_missing = polity_data_subset[polity_data_subset['iso3'].isna()]\n",
    "iso_missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4b352b-c860-4b2c-bca3-5437c2bbaf23",
   "metadata": {},
   "source": [
    "3b) Display the countries that have missing iso3. What can you tell by looking at them? Any similarities? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9760a-a4a6-4826-b377-49ff300662f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3b\n",
    "iso_missing_contries = iso_missing['country']\n",
    "print(f'They seem to be old countries, or old country names.\\n{iso_missing_contries.unique()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41b9a2-2c70-41c6-b891-95e01f2a74a5",
   "metadata": {},
   "source": [
    "3c) Display the countries with missing iso3 from 2011. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408fff2-6493-4dff-9294-ef1c564315fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3c\n",
    "# Interpreting the question so that we take the contries missing iso3 from 2011 onwards\n",
    "global df_missing_iso3_since_2011\n",
    "global df_missing_iso3_since_2011_list\n",
    "\n",
    "df_missing_iso3_since_2011 = iso_missing[iso_missing['year'].astype(int) >= 2011]\n",
    "df_missing_iso3_since_2011_list = sorted(df_missing_iso3_since_2011['country'].unique())\n",
    "df_missing_iso3_since_2011_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd9a6b6-2151-4c5e-8a8a-ea9810905642",
   "metadata": {},
   "source": [
    "3d) Display the rows for which the column \"country\" contains the word \"Serbia\". By looking at the result, can you tell what happened to Serbia in 2006? **(1 point)**\n",
    "<br>\n",
    "Hint: the most general way of doing this is to use a combination of re.search and list comprehension. To display the full subset, you can use print(df.to_string())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953ca64-d67b-4c94-8df6-5077efa68814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3d\n",
    "\n",
    "# It is important to check for varieties of capilized firstletter when using case sensitive comparison. Information may be lost. Here all country instances uses first capital letter.\n",
    "polity_data_subset_serbia = polity_data_subset[polity_data_subset['country'].str.contains(pat='Serbia', case=False)]\n",
    "print(polity_data_subset_serbia.to_string())\n",
    "\n",
    "# We can see that the country referred to as Serbia, reappeared in 2006, this is due to a name change following Montenegro and Serbias decleration of independence of their previous union named `Serbia and Montenegro`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850dd97b-0aaa-4bca-affb-22f3c0ca387c",
   "metadata": {},
   "source": [
    "3e) Write a function that does the operation in 3d and use it to display the subset that has the word \"sudan\" (all lower cap) in country. Then do the same for the word \"vietnam\" (all lower cap). **(1 point)**\n",
    "<br>\n",
    "Hint: options of functions can be very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217b330-72ef-4d38-8a66-1849d12f7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3e\n",
    "#\n",
    "# Function find_country\n",
    "# \tinput: dataframe, column, name\n",
    "# \toutput: df containting the specified string in the column 'country'\n",
    "\n",
    "def find_rows_with_pattern(df:pd.DataFrame, column:str, name:str):\n",
    "\ttry:\n",
    "\t\treturn df[df[column].str.contains(pat=name, case=False)]\n",
    "\texcept TypeError as e:\n",
    "\t\tprint(f'Not a valid datatype: {e}.\\nUse {pd.DataFrame} and {str} as option type parameters')\n",
    "\n",
    "df_sudan = find_rows_with_pattern(polity_data_subset, 'country', 'sudan')\n",
    "df_vietnam = find_rows_with_pattern(polity_data_subset, 'country', 'vietnam')\n",
    "\n",
    "print(df_sudan.to_string())\n",
    "print(df_vietnam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a0adc-6104-4dff-baa4-cbc8259183b4",
   "metadata": {},
   "source": [
    "3f) Replace nan values in iso3 with correct iso3 for the 5 countries found in 3c from 2011 onwards, and display the subset with the fixed values to check that everything worked. **(1 point)**\n",
    "<br>\n",
    "Hint: the correct iso3 for these 5 countries are \"ETH\",\"MNE\",\"SRB\",\"SDN\",\"VNM\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab9532-5a03-46fe-9db7-170e8b087957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3f\n",
    "\n",
    "# Storing the correct iso3 values in a list\n",
    "iso3_code_missing = ('ETH', 'MNE', 'SRB', 'SDN', 'VNM')\n",
    "\n",
    "# Creating a dict to look up correct iso3 values with list from 3c\n",
    "iso3_dict = dict(zip(df_missing_iso3_since_2011_list, iso3_code_missing))\n",
    "\n",
    "# Copying the politydata to work on\n",
    "polity_data_subset_iso_replace = polity_data_subset.copy()\n",
    "\n",
    "# Function: replace(df, dct, replaced)\n",
    "# input: df: dataframe, dct: dictionary to loop through, replaced: item to be replaced\n",
    "# manipulates in place\n",
    "\n",
    "def replace(df, dct, replaced, from_year):\n",
    "    for key, item in dct.items():\n",
    "        df.loc[(df['country'] == key) & (df['year'].astype(int) >= from_year)] = df.loc[(\n",
    "            df['country'] == key) & (df['year'].astype(int) >= 2011)].replace(replaced, item)\n",
    "\n",
    "\n",
    "# Calling the function and telling which value i want to replace\n",
    "replace(polity_data_subset_iso_replace, iso3_dict, np.nan, 2011)\n",
    "\n",
    "# Using the indexes of the subset created in 3c to check if we have the correct countrycodes\n",
    "polity_data_subset_iso_replace.iloc[df_missing_iso3_since_2011.index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa210c-b46f-46ff-a8da-d72d95bff196",
   "metadata": {},
   "source": [
    "3g) Drop the remaining rows which have nan in \"iso3\" and display the new number of rows of the dataframe (how many are they?) **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2cea78-0af1-405f-aeed-41dfc8fee4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3g\n",
    "\n",
    "# Dropping NaNs in the iso3 column\n",
    "polity_data_ss_cleaner = polity_data_subset_iso_replace.dropna(subset='iso3')\n",
    "\n",
    "# Number of rows, and making sure that the data is consistent\n",
    "print(f'Before removing rows with NaNs in column \"iso3\" we had: {len(polity_data_subset_iso_replace)} rows')\n",
    "print(f'Now we have: {len(polity_data_ss_cleaner)} rows')\n",
    "print(f'Which means that we removed: {len(polity_data_subset_iso_replace) - len(polity_data_ss_cleaner)} rows')\n",
    "print(f'Which makes sense since we had {polity_data_subset_iso_replace[\"iso3\"].isna().sum()} instances of NaNs in the polity2 column.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313dc318-2264-4e42-8ca3-f1cf5b85fac5",
   "metadata": {},
   "source": [
    "### Question 4: Check Polity2 <a class=\"anchor\" id=\"question4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41d643-6867-4c1c-93f9-c3c38fb9d454",
   "metadata": {},
   "source": [
    "4a) Display the first and last year included in the dataset **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57202c70-fe45-4f45-a29b-6a4761421972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4a\n",
    "\n",
    "# Assuming the question is interpreted so that we are to find earliest and latest year recorded we have:\n",
    "print(f'Earliest entry year: {polity_data[\"year\"].min()}, Latest entry year: {polity_data[\"year\"].max()}')\n",
    "\n",
    "# Assuming the assignment is asking for the first and last year in the first and last row of the original dataset\n",
    "print(f'First row year: {polity_data.iloc[[0, -1]][\"year\"].values[0]}, and last row year: {polity_data.iloc[[0, -1]][\"year\"].values[-1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4593d98-be9b-45b3-86df-82e613ee851b",
   "metadata": {},
   "source": [
    "4b) What do the values in \"polity2\" represent? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1c4ecc-c2e6-4150-bf91-86bc067fd4a7",
   "metadata": {},
   "source": [
    "Answer 4b: \n",
    "\n",
    "`Polity2` is a revised version of the Polity score, which captures a regime authority on a `21-point` scale ranging from `-10` (hereditary monarchy) to `10` (consolidated democracy). `-66` are cases of foreign interruption and are treated as missing values. Whearas `-88` represents transitions. If a given country has the value of `-5` i 1990 and `+5` in 2000, it means that it has an annual increase of `+1`; and the converted scores are `1991: -4 1992: -3 ... 2000: +5` \n",
    "\n",
    "The documentation I used is [here](https://www.systemicpeace.org/inscr/p4manualv2016.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a9072-8271-4336-9219-3c64a3f16e07",
   "metadata": {},
   "source": [
    "4c) Do we have weird values for polity2? If yes, why? What should we do about them? Transform the data accordingly. **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e710121-c355-4f34-8f81-187ccee789b7",
   "metadata": {},
   "source": [
    "Answer 4c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40f8d3-e7bd-49b9-9faa-a76931d0f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4c\n",
    "\n",
    "# We can see that we have NaNs in polity2\n",
    "print(f'Number of NaNs in the df per column: \\n {polity_data_ss_cleaner.isna().sum()}')\n",
    "print(f'\\nThe unique values of polity2: \\n {polity_data_ss_cleaner.polity2.unique()}')\n",
    "\n",
    "# To repeat:\n",
    "# -66 : cases of foreign “interruption” are treated as “system missing.”\n",
    "# -88 : Cases of “transition” are prorated across the span of the transition. \n",
    "# So the NaNs may be there for a reason...\n",
    "\n",
    "# Creating a copy df to work on\n",
    "df_cleaned = polity_data_ss_cleaner.copy()\n",
    "\n",
    "# Lets find the rows with -66 and -88\n",
    "polity_66_88_index = df_cleaned[(df_cleaned.polity2 == -66) | (df_cleaned.polity2 == -88)].index\n",
    "print(f'\\nRows with -66 and -88:\\n{polity_data_subset.iloc[polity_66_88_index].to_string()}')\n",
    "\n",
    "# In addition -88 values should be treated as missing values, as i have looked into the data for other occupied countries during the world wars\n",
    "# We see that they have NaNs and not -66. I will therefor change the -66 to NaN\n",
    "df_cleaned.loc[df_cleaned.polity2 == -66] = df_cleaned.loc[(df_cleaned.polity2 == -66)].replace(-66, 0)\n",
    "\n",
    "# As Belgium has -88 for the first row, and all the nearest following years has score -4 i will change this value to -4\n",
    "df_cleaned.loc[df_cleaned.polity2 == -88] = df_cleaned.loc[(df_cleaned.polity2 == -88)].replace(-88, np.float64(-4))\n",
    "\n",
    "# Printing the current unique polity2 values to verify changes\n",
    "print(f'\\nThe new unique values of polity2: \\n {df_cleaned.polity2.unique()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b6f3a-11d9-477f-9fab-f7e644176c67",
   "metadata": {},
   "source": [
    "4d) Make a map that shows the number of observations of polity2 by country **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53846332-57ef-4909-8367-5ffd2d737eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4d\n",
    "\n",
    "pol_obs_per_country = df_cleaned.groupby(['iso3'])['polity2'].size().reset_index()\n",
    "pol_obs_map = px.choropleth(pol_obs_per_country, locations='iso3',\n",
    "                    title='Number of polity2 observations by country',\n",
    "                    locationmode='ISO-3',\n",
    "                    color='polity2', \n",
    "                    hover_name='iso3',\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma)\n",
    "\n",
    "pol_obs_map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5258b-9ef6-4e9d-bf52-29a0d7ebc9b7",
   "metadata": {},
   "source": [
    "4e) Store the final dataframe (the one you obtained after 5d) in an object called df_pol **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb74cbc-5356-4e4b-b5b4-a5174c9ced76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4e\n",
    "df_pol = df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b6cd3-101d-4343-b76d-d97583fe33ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Quality of Government Environmental Indicators <a class=\"anchor\" id=\"qog\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58204e-f401-4b6e-b64a-c9d098faa0f2",
   "metadata": {},
   "source": [
    "The QoG Environmental Indicators dataset (QoG-EI) (Povitkina, Marina, Natalia Alvarado Pachon & Cem Mert Dalli. 2021). The Quality of Government Environmental Indicators Dataset, version Sep21. University of Gothenburg: The Quality of Government Institute, https://www.gu.se/en/quality-government), is a compilation of indicators measuring countries' environmental performance over time, including the presence and stringency of environmental policies, environmental outcomes (emissions, deforestation, etc.), and public opinion on the environment. Codebook and data are available [here](https://www.gu.se/en/quality-government/qog-data/data-downloads/environmental-indicators-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc77f2-25d1-41ce-930a-c25a05ac9cdc",
   "metadata": {},
   "source": [
    "### Question 5: Import the data and do few fixes <a class=\"anchor\" id=\"question5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959796a-3340-4231-838c-a4e2cba4262c",
   "metadata": {},
   "source": [
    "5a) Import data from the Quality of Government Environmental Indicators Dataset and display the variables types and the number of rows **(1 point)**\n",
    "<br>\n",
    "Hint: When you go on the webpage of the Environmental Indicators Dataset, you can directly import from a URL by copying the link address of the dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ef767-dca9-426e-9e34-1335038b3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5a\n",
    "url = 'https://www.qogdata.pol.gu.se/data/qog_ei_sept21.csv'\n",
    "df_qog = pd.read_csv(url, encoding='latin-1')\n",
    "\n",
    "print(f'Variable types: {df_qog.dtypes} \\n Number of rows: {len(df_qog)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a8293-d098-4a79-a803-50d52f6b63b3",
   "metadata": {},
   "source": [
    "5b) Rename the variable \"ccodealp\" to \"iso3\" **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e78d3-06b3-418c-81b8-05bbef009ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5b\n",
    "df_qog.rename(columns={'ccodealp': 'iso3'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b34e8-0daa-4481-9b1b-61e7d318c571",
   "metadata": {},
   "source": [
    "5c) Check the type of the variables \"year\" and \"iso3\" are string, if not convert them to string **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1993423-8edf-4bb0-8ea0-69034fce5a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5c\n",
    "print(f'Before convert \\nYear datatype: {df_qog.year.dtypes}')\n",
    "print(f'iso3 datatype: {df_qog.iso3.dtypes}')\n",
    "\n",
    "df_qog[['year', 'iso3']] = df_qog[['year', 'iso3']].astype('string')\n",
    "\n",
    "print(f'After convert \\nYear datatype: {df_qog.year.dtypes}')\n",
    "print(f'iso3 datatype: {df_qog.iso3.dtypes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b61d4-55aa-439b-823f-67aad3f6fc16",
   "metadata": {},
   "source": [
    "### Question 6: Merge QOG and Polity5 ... issues with QOG? <a class=\"anchor\" id=\"question6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8af7f-3b38-4834-90b4-e1cbbb7b7359",
   "metadata": {},
   "source": [
    "6a) Get a subset of the dataframe that includes the variables \"cname\", \"iso3\", \"year\" and \"cckp_temp\", and display the number of rows. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736cd36-ed4e-4dbd-96a9-c3d51bd343be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6a\n",
    "\n",
    "df_qog_subset = df_qog[['cname', 'iso3',  'year', 'cckp_temp']]\n",
    "len(df_qog_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a0373-fab5-4346-bed7-3500ed9aced0",
   "metadata": {},
   "source": [
    "6b) Merge this subset (left) and the clean version of the polity data (right), using the argument how=\"left\". Was the merge succesfull? If yes, how many rows has the merged dataframe? Is it the same number of rows of the subset in 6a? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2b0ba-fdd7-474a-9241-6d96e6eca6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6b\n",
    "\n",
    "merged = pd.merge(left=df_qog_subset, right=df_pol, how='left')\n",
    "print(f'With {len(merged)} rows, its the same amount of rows at we saw in 6a')\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e5a50-5bda-4331-82a9-9cfe09a365cb",
   "metadata": {},
   "source": [
    "6c) Do the same by adding the argument validate=\"one-to-one\". Can you make some hypotheses on why you get an error? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8e0a9-75e7-4c37-80ad-3f5ddef8d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6c\n",
    "\n",
    "# By using the option validate='one-to-one', we get an error message which states that the merge keys are not unique. \n",
    "# Which makes sense which we for example have several rows in the same column with the same value.\n",
    "merge_one_to_one = pd.merge(left=df_qog_subset, right=df_pol, validate='1:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8da9ac-de08-460e-b364-40aaf5a9aa34",
   "metadata": {},
   "source": [
    "6d) Consider the subset of the QOG you obtained in 6a and write a code to (i) count the number of observations for the variable \"cckp_temp\" for each combination of iso3 and year, (ii) store the results in a dataframe. For example, the combination \"USA-2012\" should have 1 observation for \"cckp_temp\", so the result of your code should be 1. The code should do this for all iso3-year combinations of your subset dataframe, and store the results in a dataframe. **(1 point)**\n",
    "<br>\n",
    "Hint: it should not take you more than 2 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a4be8-0760-454d-8f55-e5e0bd18af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6d\n",
    "cckp_temp_per_combination = df_qog_subset.groupby(['iso3', 'year'])['cckp_temp'].size().reset_index()\n",
    "cckp_temp_per_combination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e92628-2085-4edf-aed4-8e6db647d9c6",
   "metadata": {},
   "source": [
    "6e) Use the code in 6d to write a function that displays all rows of the dataframe obtained in 6a that have more than one observation of \"cckp_temp\" for each iso3-year combination, and check if it works. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953896a-28a5-4d38-8ad3-93027c5c4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6e\n",
    "def more_than_one_combination(df, c1, c2, obs):\n",
    "\tdf = df.groupby([c1, c2])[obs].size().reset_index()\n",
    "\tdf = df[df[obs].astype(int) > 1]\n",
    "\treturn df\n",
    "\n",
    "combination1 = 'iso3'\n",
    "combination2 = 'year'\n",
    "observation = 'cckp_temp'\n",
    "\n",
    "more_than_one_combination(df_qog_subset, combination1, combination2, observation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190da03-b34c-4566-8aec-5a1a540ac660",
   "metadata": {},
   "source": [
    "6f) Which countries have more than one observation for each iso3-year combination? Deal with these countries in the subset dataframe created in 6a to make sure you no longer have double observations for iso3-year combinations, and check that after your fix this is actually the case. **(1 point)**\n",
    "<br>\n",
    "Hint: should we keep a country with all missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d960c4a-b755-448c-8952-aee2e6eee878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6f\n",
    "\n",
    "# Using the previous function to get the countries with more than one iso3-year combination\n",
    "countries = more_than_one_combination(df_qog_subset, 'iso3', 'year', 'cckp_temp')\n",
    "\n",
    "# Getting the iso3 code for that particualar country\n",
    "iso3_duplicate_country = countries.iso3.unique()[0]\n",
    "\n",
    "# Creating a df to show all the data with the iso3-year combiantion constraints \n",
    "iso3_duplicate_df = df_qog_subset[df_qog_subset.iso3 == iso3_duplicate_country]\n",
    "\n",
    "# Inspecting the values, and seeing that all cckp_temp values for North Vietnam are NaN\n",
    "print(iso3_duplicate_df.to_string())\n",
    "\n",
    "# We saw that we only have NaN values for North Vietnam - so we lets find the indexes of these rows\n",
    "# In addiiton the Polity5 dont have any country names North Vietnam so i think we can remove these\n",
    "north_vietnam_indexes = df_qog_subset[df_qog_subset.cname == 'North Vietnam'].index\n",
    "\n",
    "# Based on the indexes we can drop these rows\n",
    "df_qog_cleaned = df_qog_subset.drop(index=north_vietnam_indexes)\n",
    "\n",
    "# Now we can check whether we have any duplicated by running the function from 6e again.\n",
    "more_than_one_combination(df_qog_cleaned,'iso3', 'year', 'cckp_temp').empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae70541-6427-4f83-bd05-37ef68baf5cd",
   "metadata": {},
   "source": [
    "6g) If your check went well, now you can perform the same operation directly in the QOG dataframe (not in the substed dataframe created in 6a). How many rows does now the QOG dataframe has? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f20b2-04e4-46d0-94f6-e580913ddfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6g\n",
    "\n",
    "# Using the function from 6e with the original dataset\n",
    "more_than_one_iso_year_combination_df = more_than_one_combination(df_qog, 'iso3', 'year', 'cckp_temp')\n",
    "\n",
    "# By inspecting the df it looks like we the same countries here. North Vietnam and Viet Nam.\n",
    "print(more_than_one_iso_year_combination_df.to_string())\n",
    "\n",
    "# Lets again find the indexes\n",
    "north_vietnam_indexes = df_qog[df_qog.cname == 'North Vietnam'].index\n",
    "\n",
    "# Dropping the North Vietnam indexes\n",
    "df_qog = df_qog.drop(index=north_vietnam_indexes)\n",
    "\n",
    "print(len(df_qog))\n",
    "\n",
    "# We can now see that we dont have any multiple combinations of iso3 and year anymore\n",
    "more_than_one_combination(df_qog, 'iso3', 'year', 'cckp_temp').empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd12e1-799e-4e4c-8d13-db496fb5bab4",
   "metadata": {},
   "source": [
    "### Question 7: Merge QOG and Polity5 ... issues with Polity5? <a class=\"anchor\" id=\"question7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72692a86-a9ab-44bb-bb56-b3c22e0829c4",
   "metadata": {},
   "source": [
    "7a) Merge the cleaned QOG dataframe (left) and the Polity dataframe (right) using the options how=\"left\" and validate=\"one_to_one\". Does it work? Why? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82261ca5-c3ec-49f6-8de8-be1b14943cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7a\n",
    "merge_qog_polity5 = pd.merge(left=df_qog, right=df_pol, how='left', validate='one_to_one')\n",
    "\n",
    "# We get the error message that the keys are not unique in the right dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95740ccf-88d7-46ee-82ab-f1a8b42d9722",
   "metadata": {},
   "source": [
    "7b) Use the function you wrote in 6e to check what's wrong in the \"clean\" version of Polity **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356409a0-73e8-41ac-af75-c2f6a462c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7b\n",
    "\n",
    "# By displaying the df_pol with iso3 value MNE we see that Kosovo has the wrong iso3 value. \n",
    "# This makes sense regarding the error message we got in 7a\n",
    "polity_combination_issues_polity = more_than_one_combination(df_pol, 'iso3', 'year', 'polity2')\n",
    "polity_combination_issues_country = more_than_one_combination(df_pol, 'iso3', 'year', 'country')\n",
    "print(f\"Checking which iso3-year combinations has several observations of polity2: \\n {polity_combination_issues_polity}\")\n",
    "print(f\"Checking which iso3-year combinations has several observations of countries: \\n {polity_combination_issues_country}\")\n",
    "print(f'Checking the countries with iso3 code MNE: {df_pol[df_pol.iso3 == \"MNE\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23d8f5b-10f5-4540-b714-99b77f3e5203",
   "metadata": {},
   "source": [
    "7c) Drop or fix the countries that create troubles directly in the \"clean\" version of Polity and motivate your choices. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb945c-418a-4eee-bdd3-2dce12246196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7c\n",
    "# Kosovo does not officially have a iso3 value but we can use XKX set by the European Commission\n",
    "# OR we can choose to drop it completely as the QoG dataset does not include data on Kosova at all\n",
    "# And since we only have values from 2011-2018 for Sudan-North and in QoG dataset dont have any\n",
    "# values for Sudan-North/North-Sudan/Sudan North/North Sudan at all, we should drop these.\n",
    "\n",
    "print(f'Checking if there are severeal countries with iso3 code SDN in QoG: {df_qog[df_qog.iso3 == \"SDN\"].cname.unique()}')\n",
    "print(f\"Checking if there is any data on Sudan-North in QoG: {find_rows_with_pattern(df_qog, 'cname', 'sudan-north').cname.unique()}\")\n",
    "print(f\"Checking if there is any data on North-Sudan in QoG: {find_rows_with_pattern(df_qog, 'cname', 'north-sudan').cname.unique()}\")\n",
    "print(f\"Checking if there is any data on North Sudan in QoG: {find_rows_with_pattern(df_qog, 'cname', 'sudan north').cname.unique()}\")\n",
    "print(f\"Checking if there is any data on Sudan North  in QoG: {find_rows_with_pattern(df_qog, 'cname', 'north sudan').cname.unique()}\")\n",
    "\n",
    "# I have devided to drop Kosovo and Sudan-North\n",
    "# Lets find the indexes of these rows\n",
    "kosovo_indexes = df_pol[df_pol.country == 'Kosovo'].index\n",
    "sudan_north_indexes = df_pol[df_pol.country == 'Sudan-North'].index\n",
    "\n",
    "# Based on the indexes we can drop these rows\n",
    "df_pol = df_pol.drop(index=kosovo_indexes)\n",
    "df_pol = df_pol.drop(index=sudan_north_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88282d66-3aa1-4fa4-b7cc-b448c649052c",
   "metadata": {},
   "source": [
    "7d) Try now to merge the \"clean-clean\" versions of qog and Polity (the ones you obtained in 7g and 8c) always using the options how=\"left\" and validate=\"one_to_one\". Does it work, and why? How many rows has the resulting merged dataframe? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e52b0f-7484-4656-82bd-8de4cee284a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7d\n",
    "merge_qog_polity5 = pd.merge(left=df_qog, right=df_pol, how='left', on=['iso3', 'year'], validate='one_to_one')\n",
    "len(merge_qog_polity5)\n",
    "\n",
    "# It works as there is no longer any ambiguioty with the combination of iso3 and year in either of the dataframes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8479c-556a-4c9b-b89f-a8f714e888ad",
   "metadata": {},
   "source": [
    "### Question 8: Clean the merged dataframe <a class=\"anchor\" id=\"question8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936854f-7882-47bc-a2c0-cb39b53128b0",
   "metadata": {},
   "source": [
    "8a) In the merged dataframe, order the columns so that you have the \"index\" variables first and the variables with actual values last. **(1 point)**\n",
    "<br>\n",
    "Hint: index variables are \"iso3\", \"year\" and other similar variables you can find, and the variables with actual values are \"polity2\", \"cckp_temp\" and other similar variables you can find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e92ad2-8007-4a4b-a47f-44c040ccfbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 8a\n",
    "\n",
    "# Displaying all the column names to get an overview\n",
    "column_list = merge_qog_polity5.columns.tolist()\n",
    "#print(column_list)\n",
    "\n",
    "# Popping the columns we want to re-order\n",
    "iso3_column = merge_qog_polity5.pop('iso3')\n",
    "year_column = merge_qog_polity5.pop('year')\n",
    "year_country = merge_qog_polity5.pop('country')\n",
    "\n",
    "# Inserting columns in the specified order\n",
    "merge_qog_polity5.insert(2, 'iso3', iso3_column)\n",
    "merge_qog_polity5.insert(3, 'year', year_column)\n",
    "merge_qog_polity5.insert(4, 'country', year_country)\n",
    "\n",
    "column_list_after = merge_qog_polity5.columns.tolist()\n",
    "#print(column_list_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b39c4e8-6f99-47c9-9d32-4196e4184537",
   "metadata": {},
   "source": [
    "8b) Rename \"cname\" as \"country\" and \"country\" as \"country_polity\". **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2cccd-473d-462e-9b2f-726aa6763d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 8b\n",
    "merge_qog_polity5.rename(columns={'cname': 'country', 'country': 'country_polity'}, inplace=True)\n",
    "merge_qog_polity5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353db63e-08d7-41df-9bdf-e0aa20174569",
   "metadata": {},
   "source": [
    "8c) Save the clean merged dataframe as a csv in a subfolder called \"clean_data\" in your working directory **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a684c82-6df7-480f-9374-a2d4e7ec069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 8c\n",
    "\n",
    "# Removing index columns\n",
    "#merge_qog_polity5 = merge_qog_polity5.pop('Unnamed: 0')\n",
    "\n",
    "# Cleaning path if exists\n",
    "if os.path.exists('clean_data'):\n",
    "\tos.remove('clean_data/clean_data.csv')\n",
    "\tos.rmdir('clean_data')\n",
    "\n",
    "# Creatning folder and exporting csv\n",
    "os.makedirs('clean_data')\n",
    "merge_qog_polity5.to_csv('clean_data/clean_data.csv', index=False)\n",
    "\n",
    "# I also want to add that i have done a lot of comparison check between the two csv files.\n",
    "# I learnt that using the .eq or .equeal method on float number does not work very well,\n",
    "# as these numbers are \"floating\", 1.0005 may in fact be 1.00005000102 when compared. \n",
    "# So i used np.isclose(a, b) - which works between a certain very small range.count\n",
    "# I'm happy to say that the two datasets looks identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f2840-c690-4b3e-8e4c-0025101bdc62",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis <a class=\"anchor\" id=\"eda\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa01e2b-dc8a-49ae-a19b-8c42bd0f2e57",
   "metadata": {},
   "source": [
    "In this section you will define a research question and perform a preliminary Exploratory Data Analysis (EDA) to address - or better, start addressing - the question at hand. This exercise will be done along the lines of the analysis done by our own Quentin Gallea in \"*A recipe to empirically answer any question quickly*\" ([Towards Data Science, 2022](https://towardsdatascience.com/a-recipe-to-empirically-answer-any-question-quickly-22e48c867dd5)). In this article, Quentin shows the first steps of an EDA that aims to explore whether heat waves have pushed governments to implement regulations against climate change (causal link). The logic is that, as it gets hotter and hotter, governments become more aware of climate change, and the problems it can cause to society, and start addressing it. In Quentin's analysis, heat waves (proxied by temperature) is the \"main explanatory variable\", rainfall is the \"explanatory variable for heterogeneity\", and regulations against climate change (proxied by the Environmental Policy Stringency Index) is the \"outcome variable\". He finds that indeed countries with relatively high temperatures have implemented more regulations against climate change. This is true especially when rainfall levels are low, as when it does not rain the damage of extreme heat is more evident to legislators, who therefore apply stricter regulations against these phenomenons.\n",
    "<br>\n",
    "<br>\n",
    "In this exercise, you will be asked to do a similar analysis on a research question of your choice, using at least two of the variables of the dataset we have created in the former questions (QOG + Polity). For example, \"what is the average temperature in 2010?\" is not a valid research question (univariate), while \"what is the impact of high temperatures on the stringency of climate regulations?\" is a valid research question (at least bivariate). As before, we will ask you some (this time more general and open) questions, and you should report your answer in the cells below each question. Use a mix of markdown and code cells to answer (markdown for text and code for graphs and tables). We should be able to run all the graphs, i.e. screenshots of graphs are not accepted. Note that for now we have put only one markdown cell and one code cell for the answer, but feel free to add as many cells as you need.\n",
    "<br>\n",
    "Beyond the python code, we will grade the interpretations of the results and the coding decision you make.\n",
    "<br>\n",
    "<br>\n",
    "Let your creativity guide you and let's have some fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f97f39-3b6d-4546-8871-b136d122854c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 9: Selecting the ingredients (how I select the variables) <a class=\"anchor\" id=\"question9\"></a>\n",
    "We have saved the clean merged data that resulted from the previous questions in \"clean_data_prepared_EDA\" (it should be the same of the one you saved in \"clean_data\"). Import the clean merged data from \"clean_data_prepared_EDA\" using this [link](https://raw.githubusercontent.com/edoardochiarotti/class_datascience/main/Notebooks/Assignment/individual_assignment/clean_data_prepared_EDA/df_qog_polity_merged.csv). Explore the variables in the newly obtained dataframe by checking the documentation of QOG and Polity. Then, define a research question that addresses a causal link between at least two of these variables. Describe the research question, why you are addressing it and the variables of interest (outcome variable, main explanatory variable and explanatory variable for heterogeneity). **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df577a5-f0f1-43e8-aeb1-0708fe53892b",
   "metadata": {},
   "source": [
    "Answer 9:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389bb830",
   "metadata": {},
   "source": [
    "#### Climate Responsibility\n",
    "\n",
    "Going back in time, from the start of the industrial revolution to now, highly industrialized countries have benefited greatly from polluting without consequence. High emission industries have created powerful nations with huge economies.  So, if we aggregate total CO2 emission per capita per country, do the high polluters take responsibility for their historic and ongoing climate impact? I want to investigate the relationship between total CO2 emission per capita and degree of environmental action. Are the high-polluting countries paying the price or are they still exploiting their influence and power to further enhance their dominance?\n",
    "\n",
    "Furthermore, one could argue that a country’s drive to incorporate environment legislation comes from selfishness. Let me clarify with an imaginative example: Let's say a highly industrialized successful country faces a lot of climate threats such as extreme weather, wildfires, and flooding. Then it is in the country’s best interest to drive environmental politics. I would in this regard call this national climate responsibility, but not global per se. So to investigate this problem I will investigate how the Climate Change Issue within each country effects the plot. The climate change issue addresses to which extent each country faces climate change.\n",
    "\n",
    "As this is a personal question of mine, I’m very curious to see if I can find any causality, and or discover other underlying driving factors. I initially wanted to use the “Environmental Policy Performance Index” but as the country availability is almost limited to the the western countries, and to withstand the use of the same variable as Quentin I will use the “Climate change related tax revenue given in % of GDP as my outcome variable. I find this suiting as it directly the relationship directly translates to “paying for wrongdoing”.\n",
    "\n",
    "So, to sum up:\n",
    "\n",
    "- Outcome: Climate change related tax revenue (% of GDP) (oecd_cctr_gdp)[^1]\n",
    "- Explanatory variable: CO2 emissions per capita (edgar_co2pc)[^2]\n",
    "- Addition Explanatory variable: Climate Change Issue Category (epi_cch)[^3]\n",
    "\n",
    "[^1]: Organisation for Economic Co-operation and Development (OECD). 2020. Policy Instruments for the Environment (PINE). url: oe.cd/pine\n",
    "\n",
    "[^2]: Schiller, Christof, Thorsten Hellmann, and Pia Paulini. 2020. “Sustainable Governance Indi-cators 2020”. Bertelsmann Stiftung. url: https://www.sgi-network.org/2020/Downloads\n",
    "\n",
    "[^3]: Wendling, Z.A. et al. 2020. “2020 Environmental Performance Index”. New Haven, CT: Yale Center for Environmental Law and Policy. url: https://epi.envirocenter.yale.edu/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b575ca1f-de5a-4b03-a51a-7f0cc5fa18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 9:\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/edoardochiarotti/class_datascience/main/Notebooks/Assignment/individual_assignment/clean_data_prepared_EDA/df_qog_polity_merged.csv'\n",
    "df_qog_polity_merged = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b5531-0329-4817-a341-d1a1bb96ef2d",
   "metadata": {},
   "source": [
    "### Question 10: Picking the right quantity of each ingredient (how I select my sample) <a class=\"anchor\" id=\"question10\"></a>\n",
    "Explore the data availability of your variables of interest and select a clean sample for the analysis. Describe this sample with the help of summary-statistics tables and maps. **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef40ba-ff3a-4983-99c7-151be2f9622e",
   "metadata": {},
   "source": [
    "Answer 10:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb7d8fb",
   "metadata": {},
   "source": [
    "The table below reveals that we have 180 observations for `epi_cch` - which is consistent with what the documentation said. It's an assessment of 8 indicators done in 2018 measuring climate challange. The score varies from 0 to 100. As this i a new variable, it may not express the historic climate challange a country faces. But as climate challanges are somewhat consistant i think that a country facing a lot of threats in 2019 may also have been so historically. So i will continue with this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb262470-32d1-4d2c-af1c-8616daed69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 10:\n",
    "df_subset = df_qog_polity_merged[['country', 'iso3', 'year', 'oecd_cctr_gdp', 'edgar_co2pc', 'epi_cch']]\n",
    "df_subset.describe()\n",
    "\n",
    "df_subset[df_subset.epi_cch.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6012f",
   "metadata": {},
   "source": [
    "Now lets see how dense our data is. Between 2000 and 2017 we have mean above 80 - meaning we have Carbon Tax % of GDP for over 80 countries. This is quite good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596be62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset[pd.notnull(df_subset['oecd_cctr_gdp'])].groupby(['year']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But since we only have data for epi_cch from 2019. I will take this data out of the dataframe, remove all NaNs and then we can merge later on\n",
    "\n",
    "climate_challange_df = df_subset[['iso3', 'year' , 'country', 'epi_cch']]\n",
    "climate_challange_clean = climate_challange_df.dropna()\n",
    "\n",
    "climate_challage_iso3_codes = climate_challange_clean.iso3.unique()\n",
    "\n",
    "# These are the countries we will use\n",
    "climate_challage_iso3_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And for the time being we can pop the epi_cch column from our Qog_polity_subset\n",
    "df_subset.pop('epi_cch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433aa29a",
   "metadata": {},
   "source": [
    "Here is the same counting table for the CO2 emission per capita. The first recordnings are from 1970. Which can make my hypothesis a bit hard to answer, as emissions before 1970 are not in this dataset. I tried looking online to see any datasets to merge in, and use. I found a reliable source at https://ourworldindata.org/co2-and-other-greenhouse-gas-emissions. My plan now is to use these values to give a more historicly correct picture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ac260",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset[pd.notnull(df_subset['edgar_co2pc'])].groupby(['year']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c13ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://nyc3.digitaloceanspaces.com/owid-public/data/co2/owid-co2-data.csv'\n",
    "df_co2 = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b0148",
   "metadata": {},
   "source": [
    "Doing some housekeeping (changing iso_code to iso3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co2 = df_co2[['country', 'year', 'iso_code', 'co2']]\n",
    "\n",
    "df_co2.rename(columns={'iso_code': 'iso3'}, inplace=True)\n",
    "df_co2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec73305",
   "metadata": {},
   "source": [
    "Lets work further with our qog_polity dataset. I want to extract the data from 2000-2018. And then sum up the accumulative Climate change related tax revenue (% of GDP) per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_subset_2000_2018 = df_subset[(df_subset.year >= 2000) & (df_subset.year <= 2018)]\n",
    "df_subset_2000_2018_slim = df_subset_2000_2018.drop(columns='edgar_co2pc')\n",
    "df_acc = df_subset_2000_2018_slim.groupby(['iso3'])['oecd_cctr_gdp'].sum().reset_index(name='Total Climate Tax by GDP')\n",
    "df_subset_2000_2018_slim.describe()\n",
    "\n",
    "# Just picking a random country to have a peek\n",
    "df_subset_2000_2018_slim[df_subset_2000_2018_slim.iso3 == 'AUS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76faae83",
   "metadata": {},
   "source": [
    "\n",
    "#### A mistake?\n",
    "...Wait a minute, how can the % of GDP from Climate Change tax be over 100% and have an mean of 76%? This seems off. I will go to the data source ( http://oe.cd/pine) and see if the values there are the same. I found this https://www.compareyourcountry.org/environmental-taxes/en/1/183/default which clearly highlights the error in our data...\n",
    "\n",
    "Then i found this site where i could download the data i wanted. At OECD (https://stats.oecd.org/Index.aspx?DataSetCode=ERTR#). I downloaded it as i coudnt find any other places where this data was accesssable by url or api.\n",
    "\n",
    "##### Its 80mb...\n",
    "\n",
    "\n",
    "So i did some adjustments to the file before commiting it to my public `data_folder` in the `class_datacience` repo on GitHub.\n",
    "At first i downloaded the whole file locally. And investigated what could be removed. I found out that there were categories for the environmental tax. All tax bases', 'Energy', 'Transport', 'Pollution', 'Resources. I'm interested in all tax bases, so that should shrink the data file a bit.\n",
    "\n",
    "I'm leaving the code i used below, but commented out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ee2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the original dataset that QoG used, its huge...\n",
    "#climate_tax = pd.read_csv('data/ERTR_29102022172652539.csv', low_memory=False)\n",
    "\n",
    "# Checking the keys\n",
    "#climate_tax.Category.unique()\n",
    "\n",
    "# Narrowing\n",
    "#climate_tax_total = climate_tax[climate_tax.Category == 'All tax bases']\n",
    "\n",
    "# Exporting\n",
    "#climate_tax_total.to_csv('oecd_climate_tax_total.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6d1742",
   "metadata": {},
   "source": [
    "That shrunk the filesize to 15mb, which is better. Ill uploead that on my public data_science repo, and then we can continue to invest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/percw/class_datascience/main/Notebooks/Assignment/individual_assignment/data/oecd_climate_tax_total.csv'\n",
    "climate_tax = pd.read_csv(url, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837df33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a lot of combinations countrycode-year combinations. \n",
    "more_than_one_combination(climate_tax, 'COU', 'Year', 'Value')\n",
    "\n",
    "# Here we can see what the keys looks like\n",
    "climate_tax.columns\n",
    "\n",
    "# I will check one country to start with to see if i can see that all of these values are about\n",
    "switzerland = climate_tax[climate_tax.Country == 'Switzerland']\n",
    "switzerland\n",
    "\n",
    "# Looks like there are different categories. I want Climate change. Lets filter on that\n",
    "switzerland_cc = switzerland[switzerland['Environmental domain'] == 'Climate change']\n",
    "switzerland_cc\n",
    "\n",
    "# And the variable im looking for is Tax revenue, % of GDP\n",
    "switzerland_cc_tax = switzerland_cc[switzerland_cc['Variable'] == 'Tax revenue, % of GDP']\n",
    "\n",
    "more_than_one_combination(switzerland_cc_tax, 'COU', 'Year', 'Value')\n",
    "# After this we dont have any ambiguity in our country-year combination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac8024",
   "metadata": {},
   "source": [
    "I have to look at the defininton in the codebook given by QoG again.\n",
    "\n",
    "> Climate change-related tax revenue as a percentage of gross domestic product (GDP). Includes\n",
    "taxes, fees and charges, tradable permits, deposit-refund systems, subsidies, and voluntary ap-\n",
    "proaches related to the domain of climate change.\n",
    "\n",
    "It looks like QoG databse used the `Tax revenue, % of total environmentally related tax revenue` value instead of `Tax revenue, % of GDP`. This value is much higer as it is the percentage of Tax related to climate change OVER environmental related tax revenue. Now it makes sense. Lets try to extract the correct values. I may be wrong here, overlooked something or misintepreted the definition. But if not - i may have found an error in the QoG database.\n",
    "\n",
    "So - to find the value that I want i need to define my subset with a couple of constraints.\n",
    "\n",
    "- Environmental domain : Climate change\n",
    "- Unit : Percentage\n",
    "- Variable : Tax revenue, % of GDP\n",
    "- CAT : TOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9685621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "climate_tax = climate_tax[(climate_tax['Environmental domain'] == 'Climate change') & (climate_tax['Unit'] == 'Percentage') & (climate_tax['Variable'] == 'Tax revenue, % of GDP') & (climate_tax['CAT'] == 'TOT')]\n",
    "\n",
    "# Great, it worked for the whole dataset as well.\n",
    "more_than_one_combination(climate_tax, 'COU', 'Year', 'Value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can start exploring the new dataset and extracting what we want\n",
    "\n",
    "ct_df = climate_tax[['COU', 'Year', 'Country', 'Value']]\n",
    "ct_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f552778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having - % seems odd. Lets look a bit further\n",
    "\n",
    "ct_df[ct_df.Value == -1.540000]\n",
    "\n",
    "# Mexico seems to have several - values.\n",
    "ct_df[ct_df.Country == 'Mexico']\n",
    "\n",
    "# Lets see if any other countries have that as well\n",
    "ct_df[ct_df.Value < 0]\n",
    "\n",
    "# Its only mexico. I dont think these data are accurate so i will remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d56b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_indexes = ct_df[ct_df.Country == 'Mexico'].index\n",
    "\n",
    "ct_df_clean = ct_df.drop(index=mexico_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602992e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now i want to rename the columns to match the QoG data\n",
    "ct_df_clean.rename(columns={'COU' : 'iso3', 'Year': 'year', 'Country' : 'country', 'Value' : 'oecd_cctr_gdp'}, inplace=True)\n",
    "ct_df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7383d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now i want to see how dense my data is\n",
    "\n",
    "ct_df_clean[pd.notnull(ct_df_clean['oecd_cctr_gdp'])].groupby(['year']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should see if there are any countries with very few tax registrations\n",
    "df_oecd_cctr_gdp_count = ct_df_clean[pd.notnull(ct_df_clean['oecd_cctr_gdp'])].groupby(['country']).size().reset_index(name=\"counts\")\n",
    "df_oecd_cctr_gdp_count\n",
    "\n",
    "# These countries has less than 15 observations, i will remove thme \n",
    "countries = df_oecd_cctr_gdp_count[df_oecd_cctr_gdp_count.counts < 15].country.unique()\n",
    "\n",
    "for c in countries:\n",
    "\tc_index = ct_df_clean[ct_df_clean.country == c].index\n",
    "\tct_df_clean = ct_df_clean.drop(index=c_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5bbfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also make the oecd tax data sub subset from 2000 to 2018\n",
    "climate_tax_df = ct_df_clean[(ct_df_clean.year >= 2000) & (ct_df_clean.year <= 2018)]\n",
    "climate_tax_df\n",
    "\n",
    "# And lets make a better name for the QOG_Polity df\n",
    "qog_pol_df = df_subset_2000_2018_slim\n",
    "qog_pol_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41b2a9",
   "metadata": {},
   "source": [
    "#### Lets aggregate\n",
    "\n",
    "So we now have 3 different datasets.\n",
    "\n",
    "- climate_tax_df : range 2000-2018. Climate Change tax % of GDP\n",
    "- climate_challange_clean : 2019. Score of Climate Challange a cnountry faces\n",
    "- df_co2 : range 1750-2020. Annual Emission per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0910ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_mean = climate_tax_df.groupby(['iso3', 'country'])['oecd_cctr_gdp'].mean().reset_index(name='mean_oecd_cctr_gdp')\n",
    "co2_total = df_co2.groupby(['iso3', 'country'])['co2'].sum().reset_index(name='total_co2')\n",
    "\n",
    "tax_mean\n",
    "\n",
    "merger1 = pd.merge(left=tax_mean, right=co2_total, on=['iso3', 'country'], how='left', validate='1:1')\n",
    "\n",
    "merger2 = pd.merge(left=merger1, right=climate_challange_clean, on=['iso3', 'country'], how='left', validate='1:1')\n",
    "\n",
    "# Now i finally got all the values i wanted\n",
    "merger2.pop('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merger2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4434c1-4ca5-44af-a24c-67a1b8d20cc6",
   "metadata": {},
   "source": [
    "### Question 11: Tasting and preparing the ingredients (univariate analysis) <a class=\"anchor\" id=\"question11\"></a>\n",
    "Do an univariate analysis for each variable you have chosen (outcome variable, main explanatory variable and explanatory variable for heterogeneity):\n",
    "- Prepare the variable, for example see if you need to transform the data further, i.e. log-transform, define a categorical variable, deal with outliers, etc.\n",
    "- Understand the nature of the variable, i.e. continuous, categorical, binary, etc., which then allows to pick the right statistical tool in the bivariate analysis.\n",
    "- Get an idea of the variable's behaviour across time and space.\n",
    "\n",
    "Describe these steps and the conclusions you can draw with the help of histograms, tables, maps and line graphs. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebcafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_obs_per_country = df_cleaned.groupby(['iso3'])['polity2'].size().reset_index()\n",
    "pol_obs_map = px.choropleth(pol_obs_per_country, locations='iso3',\n",
    "                    title='Number of polity2 observations by country',\n",
    "                    locationmode='ISO-3',\n",
    "                    color='polity2', \n",
    "                    hover_name='iso3',\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma)\n",
    "\n",
    "pol_obs_map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef687c9b-69c5-49b0-adb3-2aac7406112c",
   "metadata": {},
   "source": [
    "Answer 11:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fffac2-d33c-410f-9f10-c190a4c59641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 11:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1ad4a-70fc-4f39-bffb-cf10dd93205f",
   "metadata": {},
   "source": [
    "### Question 12: Cooking the ingredients together (bivariate analysis) <a class=\"anchor\" id=\"question12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a0c658-8c22-4bb0-b1ef-4b5860f6cc31",
   "metadata": {},
   "source": [
    "Considering the \"nature\" of your variables (continuous, categorical, binary, etc.), pick the right tool / tools for a preliminary bivariate analysis, i.e. correlation tables, bar/line graphs, scatter plots, etc. Use these tools to describe your preliminary bivariate analysis and your findings. **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8bb04c-9223-4e3d-b815-2d7ee3657f00",
   "metadata": {},
   "source": [
    "Answer 12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c0962-c661-4649-b8f3-e721e7ef66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 12:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0a05d-004a-42f3-b472-b7a808cd0dc1",
   "metadata": {},
   "source": [
    "### Question 13: Tasting the new recipe (conclusion) <a class=\"anchor\" id=\"question13\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b861d-d4d5-42b1-94ad-5aa33f9bf7a2",
   "metadata": {},
   "source": [
    "Explain what you learned, the problem faced, what would you do next (you can suggest other data you would like to have etc). **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5536aef2-a8df-4e17-9089-01ba8d93c029",
   "metadata": {},
   "source": [
    "Answer 13:\n",
    "\n",
    "FOund out that the countries do pay some tax - but is this tax money used to further reduce its impact?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d998423",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c967b75bd3da4530419091f2a82585a1448630d30358f0f328fd8248d7345ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
