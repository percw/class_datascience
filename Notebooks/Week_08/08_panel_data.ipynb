{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1b2b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGES\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "import statistics as st\n",
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "from stargazer.stargazer import Stargazer\n",
    "from linearmodels.panel import PanelOLS # conda install -c conda-forge linearmodels\n",
    "\n",
    "# FUNCTIONS FROM PACKAGES\n",
    "from numpy.linalg import inv\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# SEABORN THEME\n",
    "scale = 0.4\n",
    "W = 16*scale\n",
    "H = 9*scale\n",
    "sns.set(rc = {'figure.figsize':(W,H)})\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# WORKING DIRECTORY\n",
    "mywd = \"/Users/echiarot/Documents/GitHub/python_learn/Class Data Science\"\n",
    "os.chdir(mywd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cbc085",
   "metadata": {},
   "source": [
    "- A large strand of applied econometrics deal with cross-sections of countries, households, or firms, which are followed over time. This type of data is called panel (or longitudinal) data.\n",
    "- Definition (panel data). A longitudinal, or panel, dataset is one that follows a given sample of individuals over time, and thus provides multiple observations on each individuals of the sample. (Hsiao (2003), \"Analysis of Panel data\", second edition, p.2).\n",
    "- Note: The second dimension of panel data is typically time, and we call it \"time\" here, but it needs not to be time.\n",
    "- We call a balanced panel a dataset for which the number of time observations is the same for all countries. We will assume in this lecture that we have a balanced panel. However, in real life in most cases we will have access to unbalanced panels. This will be due to missing observations. This is usually not a problem; the theory discussed here applies to unbalanced panels as well, as long as missing observations are not missing in an \"endogenous\" way, in the sense that the fact that they are missing does not depend on the dependent variable. We will not discuss sample selection. So let's assume that the sample is balanced or that selection is \"exogenous\".\n",
    "- Note that panel data is is different from repeated (also called \"pooled\") cross-sectional data, in which we observe different countries at several points of time. Pooled cross-sections can be useful to get a larger sample and therefore more precise estimates (remember that the standard errors of OLS estimates decrease with the total sample variation of the variable).\n",
    "- Let's get some balanced data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "link = \"https://www.qogdata.pol.gu.se/data/qog_ei_sept21.xlsx\"\n",
    "df_qog = pd.read_excel(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292676cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get variables\n",
    "indexes = [\"ccodealp\",\"year\"]\n",
    "variabs_co2 = [\"edgar_co2gdp\",\"edgar_co2t\",\"edgar_co2pc\"]\n",
    "variabs_control = [\"oecd_cctr_gdp\"]\n",
    "variabs = variabs_co2 + variabs_control\n",
    "df = df_qog.loc[:,np.append(indexes,variabs)]\n",
    "\n",
    "# make gdp per capita\n",
    "df[\"gdp\"] = (df[\"edgar_co2gdp\"]/df[\"edgar_co2t\"])**(-1) # billions US dollars\n",
    "df[\"pop\"] = (df[\"edgar_co2pc\"]/df[\"edgar_co2t\"])**(-1) # millions\n",
    "df[\"gdp_pc\"] = df[\"gdp\"]/df[\"pop\"] # thousands of US dollars\n",
    "variabs = np.append(variabs, [\"gdp\",\"pop\",\"gdp_pc\"])\n",
    "\n",
    "# put ones into data\n",
    "df[\"ones\"] = 1\n",
    "\n",
    "# drop nas\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n of countries by year\n",
    "df.groupby(['year']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18dc740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 2010 to 2016\n",
    "df = df.loc[(df[\"year\"] >= 2010) & (df[\"year\"] <= 2017),:]\n",
    "df.groupby(['year']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7685df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "sns.scatterplot(x='gdp_pc', y='edgar_co2pc', data=df, color = \"r\", s = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dc039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop outliers quick and dirty\n",
    "df = df.loc[(df[\"gdp_pc\"] < 65) & (df[\"edgar_co2pc\"] < 20),:]\n",
    "df.groupby(['year']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffd4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make it balanced for simplicity\n",
    "df_agg = df.groupby(['year']).size().reset_index(name='counts')\n",
    "year_min = df_agg.loc[df_agg.counts == min(df_agg.counts),\"year\"].values\n",
    "ccodealp_sub = np.unique(df.loc[df.year == int(year_min),\"ccodealp\"].values)\n",
    "df = df.loc[np.isin(df.ccodealp, ccodealp_sub),:]\n",
    "\n",
    "df_agg = df.groupby(['ccodealp']).size().reset_index(name='counts')\n",
    "cou_drop = df_agg.loc[df_agg.counts == min(np.unique(df_agg[\"counts\"])),\"ccodealp\"].values\n",
    "df = df.loc[~np.isin(df.ccodealp, cou_drop),:]\n",
    "\n",
    "df.groupby(['year']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d83f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "sns.scatterplot(x='gdp_pc', y='edgar_co2pc', data=df, color = \"r\", s = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e43b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe logs?\n",
    "df[\"ln_gdp_pc\"] = np.log(df[\"gdp_pc\"])\n",
    "df[\"ln_edgar_co2pc\"] = np.log(df[\"edgar_co2pc\"])\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(x='ln_gdp_pc', y='ln_edgar_co2pc', data=df, color = \"r\", s = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b2c3de",
   "metadata": {},
   "source": [
    "- During last class, we have explored whether higher income leads to higher emissions, possibly through higher consumption. We have used a cross-sectional sample of 94 countries to estimate that, on average, if GDP per capita increases by 1%, we'd expect CO2 emissions per capita to increase by around 1.3245 %. This estimate comes from a specification that includes also a measure for carbon-tax revenues, as we believe that this variable can be a source of omitted-variable bias.\n",
    "- Now can our estimate still be biased? Maybe. For example, let's think about how cultural traits can lead people to generally pollute less. We could make the case that some countries have a culture of extending the life of products and re-use, at least more than other countries. This aspect may come from the way people in these countries prepare food, their religious beliefs, etc. As a result, people in these countries consume less in general, and therefore they may consume also less carbon-intensive goods and services. If these countries with such a cultural trait are also generally poorer (plausible), then we might be overestimating the impact of income on emissions. Indeed, we might be seeing a huge difference in CO2 emissions between poor and rich countries mainly because rich countries have no culture for recycling and re-using. So it is not because I am rich that I pollute more, but it is because of my culture. If I were poor, I'd still pollute a lot.\n",
    "- So how can we disentangle the impact of income per se and culture on emissions? Cultural aspects are generally unobservable, so we can't simply find a variable (observable) that controls for them. What we can do is using panel data and include countries' fixed effects.\n",
    "- Let us re-write the model for the data generating process with 2 regressors in a panel-data setup, for $i=1,...,N$ and $t=1,...,T$:\n",
    "<br><br>\n",
    "$$\n",
    "y_{it} = \\beta_0 + \\beta_1 x_{1,it} + \\beta_2 x_{2,it} + \\delta u_i + \\epsilon_{it}\n",
    "$$\n",
    "<br>\n",
    "- The new term $u_i$ refers to country-specific unobservable characteristics that might be related with $x_{2,it}$ and also affect $y_{it}$, such as the cultural aspects mentioned above. The important point is that we cannot observe them, so we can't just control for them by including them in our regression, as we did for $x_{2,it}$. So how do we do?\n",
    "- The trick is that, rather than having 1 variable for $u_i$ with data on a specific (unobservable) aspect, we can include N country dummies which equal 1 for a given country and zero otherwise, and which capture any specific characteristic of a specific country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f1a4f",
   "metadata": {},
   "source": [
    "Conceptually, for $i=1,...,N$ and $t=1,...,T$:\n",
    "<br><br>\n",
    "$$\n",
    "y_{it} = \\beta_0 + \\beta_1 x_{1,it} + \\beta_2 x_{2,it} + \\delta_1 u_{1,i} + ... + \\delta_N u_{N,i} + \\epsilon_{it}\n",
    "$$\n",
    "<br>\n",
    "- Let's write this model in expanded data-matrix notation, so we get a sense of what these dummies look like:\n",
    "<br><br>\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_{11} \\\\\n",
    "\\vdots \\\\\n",
    "y_{1T} \\\\\n",
    "- \\\\\n",
    "\\vdots \\\\\n",
    "- \\\\\n",
    "y_{N1} \\\\\n",
    "\\vdots \\\\\n",
    "y_{NT}\n",
    "\\end{bmatrix} = \n",
    "\\beta_0\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "- \\\\\n",
    "\\vdots \\\\\n",
    "- \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1\n",
    "\\end{bmatrix} +\n",
    "\\beta_1\n",
    "\\begin{bmatrix}\n",
    "x_{1,11} \\\\\n",
    "\\vdots \\\\\n",
    "x_{1,1T} \\\\\n",
    "- \\\\\n",
    "\\vdots \\\\\n",
    "- \\\\\n",
    "x_{1,N1} \\\\\n",
    "\\vdots \\\\\n",
    "x_{1,NT}\n",
    "\\end{bmatrix} +\n",
    "\\beta_2\n",
    "\\begin{bmatrix}\n",
    "x_{2,11} \\\\\n",
    "\\vdots \\\\\n",
    "x_{2,1T} \\\\\n",
    "- \\\\\n",
    "\\vdots \\\\\n",
    "- \\\\\n",
    "x_{2,N1} \\\\\n",
    "\\vdots \\\\\n",
    "x_{2,NT}\n",
    "\\end{bmatrix} +\n",
    "\\delta_1\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "- \\\\\n",
    "\\vdots \\\\\n",
    "- \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix} + ... +\n",
    "\\delta_N\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "- \\\\\n",
    "\\vdots \\\\\n",
    "- \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "\\epsilon_{11} \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_{1T} \\\\\n",
    "- \\\\\n",
    "\\vdots \\\\\n",
    "- \\\\\n",
    "\\epsilon_{N1} \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_{NT}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280ff13",
   "metadata": {},
   "source": [
    "- Or in a more compact way:\n",
    "<br><br>\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_{11} \\\\\n",
    "\\vdots \\\\\n",
    "y_{1T} \\\\\n",
    "- \\\\\n",
    "\\vdots \\\\\n",
    "- \\\\\n",
    "y_{N1} \\\\\n",
    "\\vdots \\\\\n",
    "y_{NT}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 & x_{1,11} & x_{1,11} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_{1,1T} & x_{1,1T} \\\\\n",
    "- & - & - \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "- & - & - \\\\\n",
    "1 & x_{1,N1} & x_{1,N1} \\\\\n",
    "\\vdots \\\\\n",
    "1 & x_{1,NT} & x_{1,NT}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\beta_2\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & \\dots & 0 \\\\\n",
    "- & & - \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "- & & - \\\\\n",
    "0 & \\dots & 1 \\\\\n",
    "\\vdots \\\\\n",
    "0 & \\dots & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\delta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\delta_N\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "\\epsilon_{11} \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_{1T} \\\\\n",
    "- \\\\\n",
    "\\vdots \\\\\n",
    "- \\\\\n",
    "\\epsilon_{N1} \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_{NT}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f066c",
   "metadata": {},
   "source": [
    "- Or in standard matrix notation:\n",
    "$$\n",
    "\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{U}\\boldsymbol{\\delta} + \\boldsymbol{\\epsilon}  \\qquad \\text{with } \\boldsymbol{\\epsilon} | \\boldsymbol{X} \\sim \\mathcal{N}(\\boldsymbol{0}_{NT},\\,\\sigma^2\\boldsymbol{I}_{NT})\n",
    "$$\n",
    "- Aright. So the dummies capture any country specific characteristic. If we looks at the expanded matrixes, it becomes clear what the $\\delta$ coefficients are. As you remember well, we saw that the coefficient related to a vector of ones, i.e. $\\beta_0$, is the mean of $y$ when the other covariates equal zero (intercept). In a similar way, $\\delta_1$ is the mean of $y$ for country 1 when the other covariates equal zero. Hold this thought, we'll use it in a second.\n",
    "- So cool, if we want to estimate this panel-data model, we can simply add these dummies, estimate the $\\delta$ coefficients, and we'd have controlled for any unobserved country-specific source of endogeneity. Right? Yes and no. We can definitely do it, and we will, just if we do it we might have a computational issue. Think if we have to do this for like 250 countries, or 1 million individuals. We'd need to include 250 country dummies, so our data matrixes would be huge, and inverting huge matrixes is very expensive in computation terms. Is there another way of doing an OLS estimation, in which we avoid inverting huge matrixes? It turns out that there is, and the first step to understand this methodology is starting from the Frisch-Waugh-Lovell partitioned regression theorem. In a nutshell, this theorem says that if we split our regressors in 2 matrixes, we can obtain the OLS coefficients related to the first matrix with the following steps: (i) regress $y$ on the second matrix, (ii) regress the first  matrix on the second matrix, (iii) compute the residuals of (i) and the residuals of (ii), (vi) regress the residuals obtained in (i) on the residuals obtained in (ii), and the coefficient estimates of this last step will be the coefficient estimates related to the first matrix. If we apply this to our panel-data model, we can think of $X$ as the first matrix and $U$ as the second matrix. So in our case these steps would be:\n",
    "    - Estimate $\\hat{\\boldsymbol{y}}=\\boldsymbol{U}\\hat{\\boldsymbol{\\delta}}_{1,OLS}^*$\n",
    "    - Estimate $\\hat{\\boldsymbol{X}}=\\boldsymbol{U}\\hat{\\boldsymbol{\\delta}}_{2,OLS}^*$\n",
    "    - Get residuals of first regression $\\tilde{\\boldsymbol{y}}$ and of second regression $\\tilde{\\boldsymbol{X}}$\n",
    "    - Estimate $\\tilde{\\boldsymbol{y}} = \\tilde{\\boldsymbol{X}}\\hat{\\boldsymbol{\\beta}}_{OLS}$\n",
    "- OK so that's the principle. Now it comes the trick. In the first step what we are doing is regressing $\\hat{\\boldsymbol{y}}$ only on country dummies. The coefficient estimates of these country dummies in this regression represent the country-level sample means of $\\hat{\\boldsymbol{y}}$. Remember that the mean of $\\hat{\\boldsymbol{y}}$ is the coefficient estimate related a vector of ones (mean model). So the mean of $\\hat{\\boldsymbol{y}}$ in country 1 will be the coefficient estimate related to a vector that equals 1 for country 1 and 0 otherwise. The mean of $\\hat{\\boldsymbol{y}}$ in country 2 will be the sum of the coefficient estimate for the first country and the coefficient estimate related to a vector that equals 1 for country 2 and 0 otherwise.\n",
    "- Let's do an example in which we run a regression with only year dummies as regressors (instead of country dummies, but it's the same principle). In Python, it can be done in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b251aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy reg on years\n",
    "est_ols = sm.OLS.from_formula(\"ln_edgar_co2pc ~ C(year)\", data=df).fit()\n",
    "est_ols.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e62362",
   "metadata": {},
   "source": [
    "- So the mean of `ln_edgar_co2pc` for year 2010 (automatically assigned as the first year) is 0.7153. Then, the mean of `ln_edgar_co2pc` for year 2011 is 0.7153 + 0.0138 = 0.7291. Then, the mean of `ln_edgar_co2pc` for year 2011 is 0.7153 + 0.0522 = 0.7675. Etc.\n",
    "- Let's now compute the mean by year in the standard way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87698586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean by year\n",
    "df.groupby(\"year\")[\"ln_edgar_co2pc\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b96e4fe",
   "metadata": {},
   "source": [
    "- Yes the mean of `ln_edgar_co2pc` for 2010 is 0.7153, for 2011 is 0.7291, for 2012 is 0.7675, etc.\n",
    "- OK so goign back to our steps and our country dummies. So to estimate the coefficients in our step (i), we do not have to do it through the usual OLS regression approach - which would imply inverting the dummy matrix (computationally expensive) - but we can simply obtain these estimates by computing the country-specific means for `ln_edgar_co2pc`. The same holds for step (ii), so we just need to compute the country-specific means for `ln_gdp_pc` and `oecd_cctr_gdp`. Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb0bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute means by country\n",
    "y = \"ln_edgar_co2pc\"\n",
    "X = [\"ln_gdp_pc\",\"oecd_cctr_gdp\"]\n",
    "mean_df = df.groupby(\"ccodealp\")[[y] + X].mean()\n",
    "mean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ddcb4d",
   "metadata": {},
   "source": [
    "- OK so now we have to compute the residuals. Let's store our means in vectors $\\overline{\\boldsymbol{y}}$ and  $\\overline{\\boldsymbol{X}}$ that look like this:\n",
    "<br><br>\n",
    "$$\n",
    "\\overline{\\boldsymbol{y}} = \n",
    "\\begin{bmatrix}\n",
    "\\overline{y}_{1} \\\\\n",
    "\\vdots \\\\\n",
    "\\overline{y}_{1} \\\\\n",
    "- \\\\\n",
    "\\vdots \\\\\n",
    "- \\\\\n",
    "\\overline{y}_{N} \\\\\n",
    "\\vdots \\\\\n",
    "\\overline{y}_{N} \\\\\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "\\qquad\n",
    "\\overline{\\boldsymbol{X}} = \n",
    "\\begin{bmatrix}\n",
    "\\overline{x}_{1,1} & \\overline{x}_{2,1} \\\\\n",
    "\\vdots \\\\\n",
    "\\overline{x}_{1,1} & \\overline{x}_{2,1} \\\\\n",
    "- \\\\\n",
    "\\vdots \\\\\n",
    "- \\\\\n",
    "\\overline{x}_{1,N} & \\overline{x}_{2,N} \\\\\n",
    "\\vdots \\\\\n",
    "\\overline{x}_{1,N} & \\overline{x}_{2,N} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "<br>\n",
    "- So we can obtain our residuals for steps (i) and (ii) in the following way:\n",
    "    - $\\tilde{\\boldsymbol{y}} = \\boldsymbol{y} - \\overline{\\boldsymbol{y}}$\n",
    "    - $\\tilde{\\boldsymbol{X}} = \\boldsymbol{X} - \\overline{\\boldsymbol{X}}$\n",
    "- Note that for this to hold, our $\\boldsymbol{U}$ and \\boldsymbol{X} matrixes are not exactly as the ones we have said above, as the vector of ones for the intercept is not contained in $\\boldsymbol{X}$ but is contained in $\\boldsymbol{U}$ and substitutes the first country. Pas grave (no big deal).\n",
    "- So let's do step (iii) compute the residuals for step (i) and (ii) in this clever way in Python. To do that, we need to set the indexes of our data as the country iso codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute demeaned df\n",
    "demeaned_df = (df\n",
    "               .set_index(\"ccodealp\") # set the index as the person indicator\n",
    "               [[y] + X]\n",
    "               - mean_df) # subtract the mean data\n",
    "\n",
    "demeaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5a1b2",
   "metadata": {},
   "source": [
    "- Finally, step (iv), let's regress the residuals from step (i) on the residuals of step (ii). Formally we estimate:\n",
    "<br><br>\n",
    "$$\n",
    "\\tilde{\\boldsymbol{y}} = \\tilde{\\boldsymbol{X}}\\boldsymbol{\\beta} + \\tilde{\\boldsymbol{\\epsilon}}\n",
    "$$\n",
    "<br>\n",
    "- Another way of saying that we have controlled for country dummies without actually running the regressions, is saying that we have \"wiped out\" the all unobserved (and observed) variables that are constant across time. To see that, just re-write the model in demeaned terms (we do this without the matrix notation for simplicity). For $i=1,...,N$ and $t=1,...,T$:\n",
    "<br><br>\n",
    "$$\n",
    "(y_{it} - \\overline{y}_{it}) = (\\beta_0 - \\beta_0) + \\beta_1 (x_{1,it} -  \\overline{x}_{1,it}) + \\beta_2 (x_{2,it} -  \\overline{x}_{2,it}) + \\delta (u_i - u_i) + (\\epsilon_{it} - \\overline{\\epsilon}_{it})\n",
    "$$\n",
    "<br>\n",
    "- So let's do the regression of the residuals from step (i) on the residuals from step (ii) to obtain our  estimates for $\\beta$, which we'll call fixed effect estimates obtained with the FE estimator $\\hat{\\boldsymbol{\\beta}}_{FE}$ (as they are as if we were controlling for all these country dummies):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3fb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute with ols fe estimator\n",
    "est_fe = sm.OLS.from_formula(f\"{y} ~ {'+'.join(X)}\", data=demeaned_df).fit()\n",
    "est_fe.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747e650",
   "metadata": {},
   "source": [
    "- The estimate for `ln_gdp_pc` is much lower than before! This means that, indeed, it is possible that much of the effect that we were measuring before was driven by country-specific characteristics, and not by the average effect of income on emissions. Note that the result is still statistically significant. Cool.\n",
    "- VERY IMPORTANT POINT: the interpretation of the coefficient does not change and it remains the usual one.\n",
    "- As we love matrix notation and our own functions so much, let's do the same by hand. If we want to keep using the matrix notation we used for the OLS, we need to come up with a matrix that demeans the data. Let's define $\\boldsymbol{\\iota}_T$ as a T-dimensional column vector of ones. Let us then define the following $(NT\\times NT)$ matrix $\\boldsymbol{P}$ that computes the means of data matrixes and stores them in the stacked way we saw above:\n",
    "<br><br>\n",
    "$$\n",
    "\\boldsymbol{P}=\\boldsymbol{I}_N \\otimes \\frac{1}{T}\\boldsymbol{\\iota}_T\\boldsymbol{\\iota}_T'\n",
    "$$\n",
    "<br>\n",
    "- Let's then use this matrix to compute a $(NT\\times NT)$ matrix $\\boldsymbol{Q}$ that de-means the data:\n",
    "<br><br>\n",
    "$$\n",
    "\\boldsymbol{Q}=\\boldsymbol{I}_{NT} - \\boldsymbol{P}\n",
    "$$\n",
    "<br>\n",
    "- OK so our demeaned model can be written as:\n",
    "<br><br>\n",
    "$$\n",
    "\\boldsymbol{Q}\\boldsymbol{y} = \\boldsymbol{Q}\\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{Q}\\boldsymbol{\\epsilon}\n",
    "$$\n",
    "<br>\n",
    "- Which, as by construction $\\tilde{\\boldsymbol{y}}=\\boldsymbol{Q}\\boldsymbol{y}$, $\\tilde{\\boldsymbol{X}}=\\boldsymbol{Q}\\boldsymbol{X}$, and $\\tilde{\\boldsymbol{\\epsilon}} = \\boldsymbol{Q}\\boldsymbol{\\epsilon}$, is the model we wrote before:\n",
    "<br><br>\n",
    "$$\n",
    "\\tilde{\\boldsymbol{y}} = \\tilde{\\boldsymbol{X}}\\boldsymbol{\\beta} + \\tilde{\\boldsymbol{\\epsilon}}\n",
    "$$\n",
    "<br>\n",
    "- OK let's create a FE function along the lines of our OLS function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b857d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transform panda series into vectors / matrices\n",
    "def data_to_matrix(data, variab_name):\n",
    "    \n",
    "    \"\"\" My Data to Matrix Function \"\"\"\n",
    "    \n",
    "    # store in matrixes\n",
    "    matrix = data.loc[:,variab_name].to_numpy()\n",
    "    \n",
    "    # make column vectors for arrays with less than 2 dimensions\n",
    "    if len(matrix.shape) == 1:\n",
    "        matrix = np.atleast_2d(matrix).T\n",
    "        \n",
    "    # return result\n",
    "    return matrix\n",
    "\n",
    "# FE function\n",
    "def FE_estimator(data, y, X, indexes):\n",
    "    \n",
    "    \"\"\" My Sample Mean Function \"\"\"\n",
    "    \n",
    "    # store in matrixes\n",
    "    ydata = data_to_matrix(data, variab_name = y)\n",
    "    xdata = data_to_matrix(data, variab_name = X)\n",
    "    \n",
    "    # get params\n",
    "    N = len(data[indexes[0]].unique())\n",
    "    T = len(data[indexes[1]].unique())\n",
    "    K = xdata.shape[1]\n",
    "    DF = N*T-K-N\n",
    "    \n",
    "    # demean\n",
    "    iota = np.atleast_2d(np.ones(T)).T\n",
    "    P = np.kron(np.identity(N), 1/T * iota @ iota.T)\n",
    "    Q = np.identity(N*T) - P\n",
    "    ydata_dem = Q @ ydata\n",
    "    xdata_dem = Q @ xdata\n",
    "\n",
    "    # get OLS estimate\n",
    "    betahat_FE = (inv(xdata_dem.T @ xdata_dem)) @ (xdata_dem.T @ ydata_dem)\n",
    "    \n",
    "    # get standard errors\n",
    "    resid_FE = (ydata_dem - xdata_dem @ betahat_FE)\n",
    "    sigma2hat_FE = (resid_FE.T @ resid_FE) / DF\n",
    "    betahat_FE_vcov = sigma2hat_FE * inv(xdata_dem.T @ xdata_dem)\n",
    "    sehat_FE = np.atleast_2d(np.sqrt(betahat_FE_vcov.diagonal())).T\n",
    "    \n",
    "    # get t stat\n",
    "    t_stat_FE = betahat_FE / sehat_FE\n",
    "    \n",
    "    # get p values and confidence intervals\n",
    "    \n",
    "    # create objects to store results\n",
    "    p_values_FE = np.empty((K,1,))\n",
    "    ci_low_FE = np.empty((K,1,))\n",
    "    ci_high_FE = np.empty((K,1,))\n",
    "    \n",
    "    # run loop\n",
    "    for i in range(K):\n",
    "        \n",
    "        # get p value\n",
    "        lower_area = stats.t.cdf(-abs(float(t_stat_FE[i])), df = DF)\n",
    "        upper_area = lower_area\n",
    "        p_value = lower_area + upper_area\n",
    "        p_values_FE[i] = p_value\n",
    "\n",
    "        # get confidence interval\n",
    "        alpha_inv = (1.0-0.05)\n",
    "        q1 = (1+alpha_inv)/2\n",
    "        ci_critical = stats.t.ppf(q1, DF)\n",
    "        ci_low_FE[i] = betahat_FE[i]-(ci_critical*sehat_FE[i])\n",
    "        ci_high_FE[i] = betahat_FE[i]+(ci_critical*sehat_FE[i])\n",
    "\n",
    "    # get table\n",
    "    df_res = pd.DataFrame(index=np.arange(K), columns=np.arange(7))\n",
    "    df_res.columns = [\"variable\",\"coef\",\"std err\",\"t\",\"P>|t|\",\"[0.025\",\"0.975]\"]\n",
    "    df_res[\"variable\"] = X\n",
    "    df_res[\"coef\"] = np.around(betahat_FE, 4)\n",
    "    df_res[\"std err\"] = np.around(sehat_FE, 3)\n",
    "    df_res[\"t\"] = np.around(t_stat_FE, 3)\n",
    "    df_res[\"P>|t|\"] = np.around(p_values_FE, 3)\n",
    "    df_res[\"[0.025\"] = np.around(ci_low_FE, 3)\n",
    "    df_res[\"0.975]\"] = np.around(ci_high_FE, 3)\n",
    "\n",
    "    # return\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b69ac",
   "metadata": {},
   "source": [
    "- Note that we have added an extra $N$ in the degrees of freedom correction, because all of this is as if we were estimating also $N$ country dummies (even if we don't effectively do it, we can't cheat statistics).\n",
    "- Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our function\n",
    "FE_estimator(data = df, y = [\"ln_edgar_co2pc\"], X = [\"ln_gdp_pc\",\"oecd_cctr_gdp\"], \n",
    "             indexes = [\"ccodealp\",\"year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24c8dca",
   "metadata": {},
   "source": [
    "- Let's compare it with the canned method used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# canned method\n",
    "est_fe = sm.OLS.from_formula(\"ln_edgar_co2pc ~ ln_gdp_pc + oecd_cctr_gdp - 1\", data=demeaned_df).fit()\n",
    "est_fe.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5cd604",
   "metadata": {},
   "source": [
    "- Mhm same coefficient estimates but different test statistics!!\n",
    "- Let's try with another canned method for panel OLS to see if it's us the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another canned method\n",
    "est_fe_canned = PanelOLS.from_formula(\"ln_edgar_co2pc ~ ln_gdp_pc + oecd_cctr_gdp + EntityEffects\",\n",
    "                            data=df.set_index([\"ccodealp\", \"year\"]))\n",
    "result = est_fe_canned.fit()\n",
    "result.summary.tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7a201",
   "metadata": {},
   "source": [
    "- This one gives the same of ours!! OK so there must be something in the standard canned method that is not appropriate for panel-data test statistics, maybe the degrees of freedom with demeaned data are wrong. We'll discuss this another time.\n",
    "- One thing we never covered in class is heteroscedasticity. This is a huge topic. For now let's just say that you always need to use robust standard errors, both in cross-sectional and panel-data analysis. In panel data, the most common way to have robust standard errors is clustering them, usually at the country level (or individual level). Let's do it with the canned method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ece2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# canned method with SE clustered at the country level\n",
    "est_fe_canned = PanelOLS.from_formula(\"ln_edgar_co2pc ~ ln_gdp_pc + oecd_cctr_gdp + EntityEffects\",\n",
    "                            data=df.set_index([\"ccodealp\", \"year\"]))\n",
    "\n",
    "result = est_fe_canned.fit(cov_type='clustered', cluster_entity=True)\n",
    "result.summary.tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3105c0f",
   "metadata": {},
   "source": [
    "- WOAH! When we use robust standard error (clustered at the country level) the coefficient estimate for `ln_gdp_pc` is no longer statistically significant at the 10% level!! This is huge. So basically we are saying that, after we control for country-specific unobservables, GDP per capita is no longer useful to predict CO2 emissions per capita. So it's all about country-specific unonservables, that can be culture and other stuff!! That's mind blowing.\n",
    "- To see the size of the bias, you can estimate the panel-data model without taking into account for the fixed effects. Let's do it with the canned function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae190ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute with ols pooled\n",
    "est_pool_canned = PanelOLS.from_formula(\"ln_edgar_co2pc ~ ln_gdp_pc + oecd_cctr_gdp\",\n",
    "                            data=df.set_index([\"ccodealp\", \"year\"]))\n",
    "\n",
    "result = est_pool_canned.fit(cov_type='clustered', cluster_entity=True)\n",
    "result.summary.tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d7cb4",
   "metadata": {},
   "source": [
    "- Finally, to understand better FE, let's try to visualize them.\n",
    "- Let's subset our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset\n",
    "df_plot = df[(df[\"ln_gdp_pc\"] > 2) & (df[\"ln_gdp_pc\"] < 3)].copy()\n",
    "\n",
    "# estimate ols and store fitted values\n",
    "est_ols = sm.OLS.from_formula(\"ln_edgar_co2pc ~ ln_gdp_pc\", data=df_plot).fit()\n",
    "df_plot[\"fitted_values\"] = est_ols.fittedvalues\n",
    "\n",
    "# plot observations\n",
    "sns.scatterplot(x='ln_gdp_pc', y='ln_edgar_co2pc', data=df_plot, color = \"r\", s = 20)\n",
    "\n",
    "# plot ols fit line (crossing fitted values)\n",
    "sns.lineplot(x='ln_gdp_pc', y='fitted_values', data=df_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset\n",
    "df_plot = df[(df[\"ln_gdp_pc\"] > 2) & (df[\"ln_gdp_pc\"] < 3)].copy()\n",
    "\n",
    "# estimate ols and store fitted values\n",
    "est_ols = sm.OLS.from_formula(\"ln_edgar_co2pc ~ ln_gdp_pc\", data=df_plot).fit()\n",
    "df_plot[\"fitted_values\"] = est_ols.fittedvalues\n",
    "\n",
    "# estimate fe with dummies and store fitted values\n",
    "est_fe = sm.OLS.from_formula(\"ln_edgar_co2pc ~ ln_gdp_pc + C(ccodealp)\", data=df).fit()\n",
    "df_plot[\"fitted_values_fe\"] = est_fe.fittedvalues\n",
    "\n",
    "# plot observations\n",
    "sns.scatterplot(x='ln_gdp_pc', y='ln_edgar_co2pc', data=df_plot, hue=\"ccodealp\", legend = False)\n",
    "\n",
    "# plot fe fit lines (crossing fitted values)\n",
    "for ccodealp in df_plot[\"ccodealp\"].unique():\n",
    "    df_temp = df_plot.query(f\"ccodealp=='{ccodealp}'\")\n",
    "    sns.lineplot(x='ln_gdp_pc', y='fitted_values_fe', data=df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f9f3b",
   "metadata": {},
   "source": [
    "- Take a minute to appreciate what the image above is telling you about what fixed effect is doing. Notice that fixed effect is fitting one regression line per country. Also notice that the lines are parallel. The slope of the line is the effect of GDP per capita on CO2 emissions per capita. So the fixed effect is assuming that the causal effect is constant across all countries.\n",
    "- Something that we did not cover is time fixed effects, and two-way fixed effects, which can be estimated with the canned function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a393b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
